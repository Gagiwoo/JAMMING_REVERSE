#!/usr/bin/env python3
"""
GPU ì„±ëŠ¥ ë²¤ì¹˜ë§ˆí¬ ìŠ¤í¬ë¦½íŠ¸

ì¼ë°˜ ëª¨ë“œ vs ê³ ì† ëª¨ë“œ ì„±ëŠ¥ ë¹„êµ
"""

import time
import torch
import numpy as np
from improved_trust_consensus_mappo import *

def benchmark_gpu():
    """GPU ì„±ëŠ¥ ë²¤ì¹˜ë§ˆí¬"""
    print("=" * 70)
    print("ğŸ”¥ GPU ì„±ëŠ¥ ë²¤ì¹˜ë§ˆí¬")
    print("=" * 70)
    
    # GPU ì •ë³´
    if torch.cuda.is_available():
        print(f"âœ… CUDA ì‚¬ìš© ê°€ëŠ¥")
        print(f"   GPU: {torch.cuda.get_device_name(0)}")
        print(f"   VRAM: {torch.cuda.get_device_properties(0).total_memory / 1024**3:.1f} GB")
        print(f"   CUDA Version: {torch.version.cuda}")
    else:
        print("âŒ CUDA ì‚¬ìš© ë¶ˆê°€ - CPU ëª¨ë“œ")
        return
    
    print("\n" + "=" * 70)
    print("í…ŒìŠ¤íŠ¸ 1: ë„¤íŠ¸ì›Œí¬ Forward Pass ì†ë„")
    print("=" * 70)
    
    # Actor ë„¤íŠ¸ì›Œí¬ í…ŒìŠ¤íŠ¸
    config = BASE_CONFIG.copy()
    env = CTDEMultiUAVEnv(config)
    actor = Actor(env.local_obs_dim, env.action_dim, hidden=128).to(DEVICE)
    
    # ë‹¤ì–‘í•œ ë°°ì¹˜ í¬ê¸° í…ŒìŠ¤íŠ¸
    batch_sizes = [1, 10, 50, 100, 500, 1000]
    
    for batch_size in batch_sizes:
        # ëœë¤ ì…ë ¥ ìƒì„±
        dummy_input = torch.randn(batch_size, env.local_obs_dim, device=DEVICE)
        
        # Warm-up
        for _ in range(10):
            _ = actor(dummy_input)
        
        # ë²¤ì¹˜ë§ˆí¬
        torch.cuda.synchronize()
        start = time.time()
        
        iterations = 100
        for _ in range(iterations):
            _ = actor(dummy_input)
        
        torch.cuda.synchronize()
        elapsed = time.time() - start
        
        throughput = (batch_size * iterations) / elapsed
        
        print(f"  Batch Size {batch_size:4d}: {throughput:8.0f} samples/sec ({elapsed*1000/iterations:.2f} ms/iter)")
    
    print("\n" + "=" * 70)
    print("í…ŒìŠ¤íŠ¸ 2: ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰")
    print("=" * 70)
    
    torch.cuda.empty_cache()
    torch.cuda.reset_peak_memory_stats()
    
    # í° ë°°ì¹˜ë¡œ ë©”ëª¨ë¦¬ í…ŒìŠ¤íŠ¸
    large_batch = 2000
    dummy_input = torch.randn(large_batch, env.local_obs_dim, device=DEVICE)
    _ = actor(dummy_input)
    
    allocated = torch.cuda.memory_allocated(0) / 1024**2
    reserved = torch.cuda.memory_reserved(0) / 1024**2
    peak = torch.cuda.max_memory_allocated(0) / 1024**2
    
    print(f"  í˜„ì¬ í• ë‹¹: {allocated:.1f} MB")
    print(f"  ì˜ˆì•½ë¨:     {reserved:.1f} MB")
    print(f"  ìµœëŒ€ ì‚¬ìš©:  {peak:.1f} MB")
    
    print("\n" + "=" * 70)
    print("í…ŒìŠ¤íŠ¸ 3: CPU vs GPU ë¹„êµ")
    print("=" * 70)
    
    batch_size = 100
    iterations = 100
    
    # CPU í…ŒìŠ¤íŠ¸
    actor_cpu = Actor(env.local_obs_dim, env.action_dim, hidden=128).to('cpu')
    dummy_input_cpu = torch.randn(batch_size, env.local_obs_dim)
    
    start = time.time()
    for _ in range(iterations):
        _ = actor_cpu(dummy_input_cpu)
    cpu_time = time.time() - start
    
    # GPU í…ŒìŠ¤íŠ¸
    dummy_input_gpu = torch.randn(batch_size, env.local_obs_dim, device=DEVICE)
    torch.cuda.synchronize()
    start = time.time()
    for _ in range(iterations):
        _ = actor(dummy_input_gpu)
    torch.cuda.synchronize()
    gpu_time = time.time() - start
    
    speedup = cpu_time / gpu_time
    
    print(f"  CPU: {cpu_time:.3f}ì´ˆ ({batch_size*iterations/cpu_time:.0f} samples/sec)")
    print(f"  GPU: {gpu_time:.3f}ì´ˆ ({batch_size*iterations/gpu_time:.0f} samples/sec)")
    print(f"  âš¡ ì†ë„ í–¥ìƒ: {speedup:.1f}x")
    
    print("\n" + "=" * 70)
    print("í…ŒìŠ¤íŠ¸ 4: ì‹¤ì œ í•™ìŠµ ì‹œë®¬ë ˆì´ì…˜ (10 ìŠ¤í…)")
    print("=" * 70)
    
    agent = MAPPOAgentWithTrust(env.local_obs_dim, env.global_obs_dim, env.action_dim, config)
    
    # ì§§ì€ í•™ìŠµ ì‹œë®¬ë ˆì´ì…˜
    scenario = EnvironmentScenario(config)
    lo, go = env.reset_with_scenario(scenario)
    agent.reset_episode(env.agents)
    
    start = time.time()
    
    for step in range(10):
        acts, logs, val, _ = agent.select_action(lo, go, env.uav_positions, env.gps_positions, env=env)
        lo, go, rew, done, info = env.step(acts)
        
        if done:
            break
    
    elapsed = time.time() - start
    
    print(f"  10 ìŠ¤í… ì‹¤í–‰ ì‹œê°„: {elapsed:.3f}ì´ˆ")
    print(f"  ìŠ¤í…ë‹¹ í‰ê· : {elapsed/10*1000:.1f} ms")
    print(f"  ì˜ˆìƒ ì—í”¼ì†Œë“œ ì‹œê°„ (200 ìŠ¤í…): {elapsed*20:.1f}ì´ˆ")
    
    print("\n" + "=" * 70)
    print("ğŸ’¡ ìµœì í™” ê¶Œì¥ì‚¬í•­")
    print("=" * 70)
    
    # GPU í™œìš©ë„ ì¶”ì •
    if speedup < 5:
        print("âš ï¸ GPU ê°€ì†ì´ ì¶©ë¶„í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤")
        print("   â†’ ë°°ì¹˜ í¬ê¸°ë¥¼ ëŠ˜ë ¤ë³´ì„¸ìš” (episodes_per_batch: 20)")
        print("   â†’ í™˜ê²½ ë³‘ë ¬í™”ë¥¼ ê³ ë ¤í•˜ì„¸ìš”")
    else:
        print("âœ… GPU ê°€ì†ì´ ì˜ ì‘ë™í•˜ê³  ìˆìŠµë‹ˆë‹¤")
    
    if allocated < 500:  # 500MB ë¯¸ë§Œ
        print("âš ï¸ GPU ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ì´ ì ìŠµë‹ˆë‹¤")
        print("   â†’ ë°°ì¹˜ í¬ê¸°ë¥¼ ë” ëŠ˜ë¦´ ìˆ˜ ìˆìŠµë‹ˆë‹¤")
        print("   â†’ ë„¤íŠ¸ì›Œí¬ í¬ê¸°ë¥¼ í‚¤ìš¸ ìˆ˜ ìˆìŠµë‹ˆë‹¤")
    
    # ì˜ˆìƒ í•™ìŠµ ì‹œê°„ ê³„ì‚°
    steps_per_episode = 150
    episodes_total = 10000
    episodes_per_batch = 10
    
    time_per_episode = elapsed * (steps_per_episode / 10)
    total_time_hours = (time_per_episode * episodes_total) / 3600
    
    print(f"\nğŸ“Š ì˜ˆìƒ ì „ì²´ í•™ìŠµ ì‹œê°„ (10,000 ì—í”¼ì†Œë“œ):")
    print(f"   ì¼ë°˜ ëª¨ë“œ: {total_time_hours:.1f} ì‹œê°„")
    
    # ìµœì í™” í›„ ì˜ˆìƒ ì‹œê°„
    optimized_time = total_time_hours * 0.4  # 60% ê°ì†Œ ì˜ˆìƒ
    print(f"   ê³ ì† ëª¨ë“œ: {optimized_time:.1f} ì‹œê°„ (ì˜ˆìƒ)")
    print(f"   âš¡ ì ˆì•½ ì‹œê°„: {total_time_hours - optimized_time:.1f} ì‹œê°„")
    
    print("\n" + "=" * 70)


def monitor_gpu_usage():
    """
    ì‹¤ì‹œê°„ GPU ì‚¬ìš©ë¥  ëª¨ë‹ˆí„°ë§ (nvidia-smi ëŒ€ì‹ )
    """
    print("\nì‹¤ì‹œê°„ GPU ëª¨ë‹ˆí„°ë§ (Ctrl+Cë¡œ ì¢…ë£Œ)")
    print("-" * 70)
    print(f"{'Time':<12} {'Memory Used':<15} {'Memory Total':<15} {'Utilization':<15}")
    print("-" * 70)
    
    try:
        while True:
            mem_alloc = torch.cuda.memory_allocated(0) / 1024**2
            mem_reserved = torch.cuda.memory_reserved(0) / 1024**2
            mem_total = torch.cuda.get_device_properties(0).total_memory / 1024**2
            
            util = (mem_alloc / mem_total) * 100
            
            timestamp = time.strftime("%H:%M:%S")
            print(f"{timestamp:<12} {mem_alloc:>6.1f} MB      {mem_total:>6.1f} MB      {util:>5.1f}%", end='\r')
            
            time.sleep(0.5)
    except KeyboardInterrupt:
        print("\n\nëª¨ë‹ˆí„°ë§ ì¢…ë£Œ")


def quick_test():
    """ë¹ ë¥¸ í•™ìŠµ í…ŒìŠ¤íŠ¸ (1ë¶„)"""
    print("\n" + "=" * 70)
    print("ğŸš€ ë¹ ë¥¸ í•™ìŠµ í…ŒìŠ¤íŠ¸ (1ë¶„)")
    print("=" * 70)
    
    config = BASE_CONFIG.copy()
    config.update(ALGORITHM_CONFIGS["Trust+Consensus-MAPPO"])
    config["total_episodes"] = 10
    config["episodes_per_batch"] = 2
    config["num_obstacles"] = 20
    config["max_steps"] = 50
    
    env = CTDEMultiUAVEnv(config)
    agent = MAPPOAgentWithTrust(env.local_obs_dim, env.global_obs_dim, env.action_dim, config)
    
    print("ì„¤ì •:")
    print(f"  Episodes: {config['total_episodes']}")
    print(f"  UAVs: {config['num_uavs']}")
    print(f"  Max Steps: {config['max_steps']}")
    
    start_time = time.time()
    
    for ep in range(0, config["total_episodes"], config["episodes_per_batch"]):
        print(f"\në°°ì¹˜ {ep//config['episodes_per_batch'] + 1}/{config['total_episodes']//config['episodes_per_batch']}")
        
        for _ in range(config["episodes_per_batch"]):
            scenario = EnvironmentScenario(config)
            lo, go = env.reset_with_scenario(scenario)
            agent.reset_episode(env.agents)
            done = False
            
            ep_obs, ep_glo, ep_act, ep_logp, ep_val, ep_rew, ep_done = [],[],[],[],[],[],[]
            
            while not done:
                acts, logs, val, _ = agent.select_action(lo, go, env.uav_positions, env.gps_positions, env=env)
                n_lo, n_go, rew, done, info = env.step(acts)
                
                ep_obs.extend([lo[a] for a in env.agents if a in acts])
                ep_glo.extend([go for _ in acts])
                ep_act.extend(list(acts.values()))
                ep_logp.extend(list(logs.values()))
                ep_val.extend([val for _ in acts])
                ep_rew.extend(list(rew.values()))
                ep_done.extend([done for _ in acts])
                
                lo, go = n_lo, n_go
            
            agent.buffer.add(ep_obs, ep_glo, ep_act, ep_logp, ep_val, ep_rew, ep_done)
            print(f"  ì—í”¼ì†Œë“œ ì™„ë£Œ: Success={info['success_rate']:.0%}")
        
        # ì—…ë°ì´íŠ¸
        with torch.no_grad():
            next_val = agent.critic(torch.tensor(go, dtype=torch.float32, device=DEVICE).unsqueeze(0)).item()
        agent.compute_gae(next_val)
        agent.update()
        print(f"  ë„¤íŠ¸ì›Œí¬ ì—…ë°ì´íŠ¸ ì™„ë£Œ")
    
    elapsed = time.time() - start_time
    
    print("\n" + "=" * 70)
    print(f"âœ… í…ŒìŠ¤íŠ¸ ì™„ë£Œ: {elapsed:.1f}ì´ˆ")
    print(f"   ì—í”¼ì†Œë“œë‹¹ í‰ê· : {elapsed/config['total_episodes']:.1f}ì´ˆ")
    
    # ì „ì²´ í•™ìŠµ ì‹œê°„ ì˜ˆì¸¡
    full_episodes = 10000
    predicted_time = (elapsed / config['total_episodes']) * full_episodes / 3600
    
    print(f"\nğŸ“Š 10,000 ì—í”¼ì†Œë“œ ì˜ˆìƒ ì‹œê°„: {predicted_time:.1f} ì‹œê°„")
    print("=" * 70)


if __name__ == "__main__":
    import sys
    
    if len(sys.argv) > 1:
        if sys.argv[1] == "monitor":
            monitor_gpu_usage()
        elif sys.argv[1] == "quick":
            quick_test()
        else:
            print("Usage:")
            print("  python benchmark_gpu.py          - ì „ì²´ ë²¤ì¹˜ë§ˆí¬")
            print("  python benchmark_gpu.py monitor  - GPU ëª¨ë‹ˆí„°ë§")
            print("  python benchmark_gpu.py quick    - ë¹ ë¥¸ í•™ìŠµ í…ŒìŠ¤íŠ¸")
    else:
        benchmark_gpu()
