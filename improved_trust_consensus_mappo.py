# improved_trust_consensus_mappo.py
"""
GPS ìŠ¤í‘¸í•‘ í™˜ê²½ì—ì„œ ì‹ ë¢°ë„ ê¸°ë°˜ ë‹¤ì¤‘ UAV í˜‘ë ¥ ê²½ë¡œ ê³„íš
Trust-based Cooperative Path Planning for Multi-UAV Systems under GPS Spoofing Attacks

ë…¼ë¬¸ ëª…ì„¸ì— ë§ê²Œ ê°œì„ ëœ ë²„ì „
Author: ê¹€ë„ìœ¤ (ë…¼ë¬¸ ì €ì)
Improved by: AI Code Reviewer

ì£¼ìš” ê°œì„ ì‚¬í•­:
1. Trust Network ì•„í‚¤í…ì²˜: 3ì¸µ Ã— 16 ë‰´ëŸ° (ë…¼ë¬¸ ëª…ì„¸ ì¤€ìˆ˜)
2. Actor ë„¤íŠ¸ì›Œí¬: ë¶ˆí•„ìš”í•œ ì¸µ ì œê±° (1 hidden layer)
3. Consensus Protocol: 50% íˆ¬í‘œ ê¸°ë°˜ ê°•ì œ ì„¤ì • ë©”ì»¤ë‹ˆì¦˜ ì¶”ê°€
4. í•˜ì´í¼íŒŒë¼ë¯¸í„°: ë…¼ë¬¸ ëª…ì„¸ì™€ ì •í™•íˆ ì¼ì¹˜í•˜ë„ë¡ ìˆ˜ì •
5. ê´€ì°° ê³µê°„: ìœµí•©ëœ ìœ„ì¹˜ ì‚¬ìš© ë° ì†ë„ ì¶”ê°€
6. GPS ê³µê²© í™•ë¥ : 10%ë¡œ ìˆ˜ì •
"""

import os
import sys
import time
import threading
import queue
import random
import warnings
import csv
import datetime
from collections import deque
from copy import deepcopy
import numpy as np
import pygame
import torch
import torch.nn as nn
import torch.nn.functional as F
import torch.optim as optim
from torch.distributions import Categorical
from torch.utils.tensorboard import SummaryWriter
from PySide6.QtWidgets import *
from PySide6.QtCore import QTimer, Qt
from PySide6.QtGui import QTextCursor, QFont
import qdarkstyle
from matplotlib.figure import Figure
from matplotlib.backends.backend_qtagg import FigureCanvasQTAgg as FigureCanvas
import matplotlib.pyplot as plt

warnings.filterwarnings('ignore', category=UserWarning, module='matplotlib.font_manager')

# ==================== CONFIG (ë…¼ë¬¸ ëª…ì„¸ ì¤€ìˆ˜) ====================
BASE_CONFIG = {
    # ---------------- ë³´ìƒ ì„¤ì • ----------------
    "reward_goal": 50.0,
    "reward_team_success": 20.0,
    "reward_collision": -10.0,
    "reward_step_penalty": -0.1,  # ë…¼ë¬¸: -0.1 per step
    "distance_reward_factor": 0.1,
    
    # ---------------- í•™ìŠµ í•˜ì´í¼íŒŒë¼ë¯¸í„° (ë…¼ë¬¸ ëª…ì„¸) ----------------
    "mappo_lr": 3e-4,  # âœ… ìˆ˜ì •: 5e-4 â†’ 3e-4
    "trust_lr": 1.5e-4,  # âœ… ì¶”ê°€: Trust Network Learning Rate (50% of Actor)
    "mappo_entropy": 0.01,
    "gamma": 0.99,
    "gae_lambda": 0.95,
    "ppo_clip_epsilon": 0.2,
    "update_epochs": 10,
    "batch_size": 512,
    
    # ---------------- í™˜ê²½ ì„¤ì • ----------------
    "num_uavs": 10,
    "grid_size": 40,
    "num_obstacles": 40,
    "max_steps": 200,
    "vision_range": 5,
    
    # ---------------- ê³µê²© ì„¤ì • (ë…¼ë¬¸ ëª…ì„¸) ----------------
    "attack_prob": 0.1,  # âœ… ìˆ˜ì •: 5% â†’ 10%
    "attack_mode": "hybrid",
    "attack_start_prob": 0.1,  # âœ… ìˆ˜ì •: 0.05 â†’ 0.1 (10%)
    "attack_min_duration": 10,
    "attack_max_duration": 30,
    
    # ---------------- Trust Network ì„¤ì • (ë…¼ë¬¸ í•µì‹¬) ----------------
    "use_trust_network": True,
    "trust_hidden": 16,  # âœ… ìˆ˜ì •: 32 â†’ 16
    "trust_lambda_reg": 0.1,  # âœ… ìˆ˜ì •: 0.05 â†’ 0.1
    
    # ---------------- Consensus ì„¤ì • (ë…¼ë¬¸ í•µì‹¬) ----------------
    "use_consensus": True,
    "consensus_threshold": 2.5,  # âœ… ìˆ˜ì •: 2.0 â†’ 2.5 cells
    "consensus_weight": 0.15,  # âœ… ìˆ˜ì •: 0.2 â†’ 0.15
    "consensus_vote_threshold": 0.5,  # âœ… ì¶”ê°€: 50% íˆ¬í‘œ ì„ê³„ê°’
    
    # ---------------- LSTM ê¸°ë°˜ ìŠ¤í‘¸í•‘ ë³´ì •ê¸° ì„¤ì • ----------------
    "detector_seq_len": 10,
    "detector_feature_dim": 5,
    "detector_hidden": 64,
    
    # ---------------- í•™ìŠµ ì œì–´ ----------------
    "total_episodes": 10000,
    "episodes_per_batch": 10,
    "render_delay": 0.1,
    "demo_episodes": 3,
    "checkpoint_interval": 5000,
    "seed": 42,
}

DEVICE = torch.device("cuda" if torch.cuda.is_available() else "cpu")

ALGORITHM_CONFIGS = {
    "Vanilla-MAPPO": {
        "use_trust_network": False,
        "use_consensus": False,
        "use_lstm_detection": False,
        "description": "ê¸°ë³¸ MAPPO (Baseline, ë¬´ë°©ë¹„)"
    },
    "LSTM-MAPPO": {
        "use_trust_network": False,
        "use_consensus": False,
        "use_lstm_detection": True,
        "description": "LSTM ê¸°ë°˜ (ê¸°ì¡´ ì—°êµ¬, ì‹œê³„ì—´ ì˜ì¡´)"
    },
    "Trust-MAPPO": {
        "use_trust_network": True,
        "use_consensus": False,
        "use_lstm_detection": False,
        "description": "ì‹ ë¢°ë„ í•™ìŠµë§Œ (Ablation)"
    },
    "Trust+Consensus-MAPPO": {
        "use_trust_network": True,
        "use_consensus": True,
        "use_lstm_detection": False,
        "description": "ì œì•ˆ ê¸°ë²• (Ours, Full) - ë…¼ë¬¸"
    },
    "LSTM-Detector-MAPPO": {
        "use_trust_network": False,
        "use_consensus": False,
        "use_lstm_detection": False,
        "use_spoof_lstm_detector": True,
        "description": "LSTM ê¸°ë°˜ GPS ìŠ¤í‘¸í•‘ ë³´ì • baseline"
    }
}

# ==================== UTILS ====================
def create_model_folder_name(config, algorithm):
    timestamp = int(time.time())
    folder_name = f"RobustRL_{algorithm}_{config['attack_mode']}_obs{config['num_obstacles']}_{timestamp}"
    return folder_name

# ==================== NETWORKS (ë…¼ë¬¸ ëª…ì„¸ ì¤€ìˆ˜) ====================

class TrustNetwork(nn.Module):
    """
    âœ… ê°œì„ : ë…¼ë¬¸ ëª…ì„¸ì— ë§ê²Œ ìˆ˜ì •
    - 3ê°œì˜ ì€ë‹‰ì¸µ, ê° 16 ë‰´ëŸ°
    - ì…ë ¥: 4ì°¨ì› (temporal_residual, spatial_discrepancy, gps_variance, vision_quality)
    - ì¶œë ¥: 2ì°¨ì› (GPS trust, Vision trust)
    """
    def __init__(self, hidden=16):
        super().__init__()
        self.network = nn.Sequential(
            nn.Linear(4, hidden), nn.ReLU(),      # Layer 1: 4 â†’ 16
            nn.Linear(hidden, hidden), nn.ReLU(), # Layer 2: 16 â†’ 16
            nn.Linear(hidden, hidden), nn.ReLU(), # Layer 3: 16 â†’ 16 (âœ… ì¶”ê°€)
            nn.Linear(hidden, 2),                 # Output: 16 â†’ 2
            nn.Softmax(dim=-1)
        )
    
    def forward(self, trust_features):
        """
        Args:
            trust_features: (batch_size, 4) tensor
                - normalized temporal residual
                - normalized spatial discrepancy
                - GPS variance
                - Vision quality (1 if neighbors exist, else 0)
        Returns:
            trust_scores: (batch_size, 2) tensor [GPS_trust, Vision_trust]
        """
        return self.network(trust_features)


class TrustLoss:
    """
    ë…¼ë¬¸ ìˆ˜ì‹: Loss = MSE(p_fused, p_real) + Î» * MSE(trust_t, trust_{t-1})
    Smoothness Regularizationì„ í†µí•œ ì•ˆì •ì ì¸ ì‹ ë¢°ë„ í•™ìŠµ
    """
    def __init__(self, lambda_reg=0.1):  # âœ… ìˆ˜ì •: 0.05 â†’ 0.1
        self.lambda_reg = lambda_reg
    
    def compute(self, fused_pos, real_pos, current_trust, prev_trust):
        """
        Args:
            fused_pos: ìœµí•©ëœ ìœ„ì¹˜ (batch_size, 2)
            real_pos: ì‹¤ì œ ìœ„ì¹˜ (batch_size, 2)
            current_trust: í˜„ì¬ ì‹ ë¢°ë„ ì ìˆ˜ (batch_size, 2)
            prev_trust: ì´ì „ ì‹ ë¢°ë„ ì ìˆ˜ (batch_size, 2)
        Returns:
            total_loss: Fusion Loss + Î» * Smoothness Loss
        """
        fusion_loss = torch.mean((fused_pos - real_pos) ** 2)
        smoothness_loss = torch.mean((current_trust - prev_trust) ** 2)
        return fusion_loss + self.lambda_reg * smoothness_loss


class ConsensusProtocol:
    """
    âœ… ê°œì„ : ë…¼ë¬¸ ëª…ì„¸ì— ë§ê²Œ 50% íˆ¬í‘œ ê¸°ë°˜ ê°•ì œ ì„¤ì • ë©”ì»¤ë‹ˆì¦˜ ì¶”ê°€
    
    ë…¼ë¬¸ ì•Œê³ ë¦¬ì¦˜:
    1. ê° UAVëŠ” ì´ì›ƒë“¤ì˜ GPS ìœ„ì¹˜ì™€ Vision ê´€ì¸¡ì„ ë¹„êµ
    2. ë¶ˆì¼ì¹˜ê°€ thresholdë¥¼ ì´ˆê³¼í•˜ë©´ ì˜ì‹¬ í‘œ(suspicion vote) ë¶€ì—¬
    3. ì „ì²´ ì´ì›ƒì˜ 50% ì´ìƒì—ê²Œì„œ ì˜ì‹¬ í‘œë¥¼ ë°›ìœ¼ë©´ GPS ì‹ ë¢°ë„ë¥¼ ê°•ì œë¡œ 0ìœ¼ë¡œ ì„¤ì •
    """
    def __init__(self, threshold=2.5, consensus_weight=0.15, vote_threshold=0.5):
        self.threshold = threshold  # âœ… ìˆ˜ì •: 2.0 â†’ 2.5
        self.consensus_weight = consensus_weight  # âœ… ìˆ˜ì •: 0.2 â†’ 0.15
        self.vote_threshold = vote_threshold  # âœ… ì¶”ê°€: 50% íˆ¬í‘œ ì„ê³„ê°’
    
    def compute_discrepancy(self, my_vision_obs, neighbor_gps_claim):
        """ì´ì›ƒì˜ GPS ìœ„ì¹˜ì™€ ë‚´ Vision ê´€ì¸¡ ê°„ ë¶ˆì¼ì¹˜ ê³„ì‚°"""
        return np.linalg.norm(my_vision_obs - neighbor_gps_claim)
    
    def cast_votes(self, discrepancies):
        """
        âœ… ì¶”ê°€: ê° ì´ì›ƒì— ëŒ€í•œ ì˜ì‹¬ í‘œ ë¶€ì—¬
        
        Args:
            discrepancies: List of discrepancies for each neighbor
        Returns:
            suspicion_votes: List of binary votes (1 if suspicious, 0 otherwise)
        """
        votes = []
        for disc in discrepancies:
            if disc > self.threshold:
                votes.append(1)  # ì˜ì‹¬ í‘œ
            else:
                votes.append(0)  # ì •ìƒ í‘œ
        return votes
    
    def aggregate_votes(self, votes_received):
        """
        âœ… ì¶”ê°€: ë°›ì€ ì˜ì‹¬ í‘œ ì§‘ê³„ ë° ê³µê²© ì—¬ë¶€ íŒë‹¨
        
        Args:
            votes_received: Number of suspicion votes received from neighbors
            total_neighbors: Total number of neighbors
        Returns:
            is_under_attack: True if votes_received >= 50% of total_neighbors
        """
        if len(votes_received) == 0:
            return False, 0.0
        
        suspicion_ratio = sum(votes_received) / len(votes_received)
        is_under_attack = suspicion_ratio >= self.vote_threshold
        return is_under_attack, suspicion_ratio
    
    def adjust_trust(self, trust_gps, trust_vis, consensus_vote, force_zero=False):
        """
        âœ… ê°œì„ : 50% íˆ¬í‘œ ê¸°ë°˜ ê°•ì œ ì„¤ì • ì¶”ê°€
        
        Args:
            trust_gps: Current GPS trust score
            trust_vis: Current Vision trust score
            consensus_vote: Aggregated consensus vote (average discrepancy or ratio)
            force_zero: If True, force GPS trust to 0 (collective decision)
        Returns:
            adjusted_trust_gps, adjusted_trust_vis
        """
        # âœ… ì¶”ê°€: ì§‘ë‹¨ ì˜ì‚¬ê²°ì •ì— ì˜í•œ ê°•ì œ ì„¤ì •
        if force_zero:
            trust_gps = 0.0
            trust_vis = 1.0
            return trust_gps, trust_vis
        
        # ê¸°ì¡´ ë¶€ë“œëŸ¬ìš´ ì¡°ì • (ê³µê²©ì´ ê°ì§€ë˜ì§€ ì•Šì€ ê²½ìš°)
        ratio = np.clip(consensus_vote / self.threshold, 0.0, 2.0)
        
        if ratio > 0.8:  # ê³µê²© ì˜ì‹¬ ê°•í™”
            delta = (ratio - 0.8) * self.consensus_weight * 1.5
            trust_gps *= (1 - delta)
            trust_vis *= (1 + delta)
        elif ratio < 0.4:  # GPS ì‹ ë¢°ë„ ë³µêµ¬
            delta = (0.4 - ratio) * self.consensus_weight * 0.5
            trust_gps *= (1 + delta)
            trust_vis *= (1 - delta)
        
        # ë²”ìœ„ ì œí•œ
        trust_gps = np.clip(trust_gps, 0.01, 0.99)
        trust_vis = np.clip(trust_vis, 0.01, 0.99)
        
        # ì •ê·œí™”
        total = trust_gps + trust_vis
        return trust_gps / total, trust_vis / total


class LSTMSpoofDetector(nn.Module):
    """
    LSTM ê¸°ë°˜ GPS ìŠ¤í‘¸í•‘ ë³´ì •ê¸° (Baseline ë¹„êµìš©)
    Residual Learningì„ í†µí•œ ìœ„ì¹˜ ë³´ì •
    """
    def __init__(self, feature_dim=5, hidden=64):
        super().__init__()
        self.lstm = nn.LSTM(feature_dim, hidden, batch_first=True)
        self.fc = nn.Linear(hidden, 2)
        # ì‘ì€ ì´ˆê¸°ê°’ìœ¼ë¡œ ì•ˆì •ì ì¸ í•™ìŠµ
        nn.init.uniform_(self.fc.weight, -0.001, 0.001)
        nn.init.constant_(self.fc.bias, 0)

    def forward(self, x):
        """
        Args:
            x: (batch_size, seq_len, feature_dim)
        Returns:
            correction: (batch_size, 2) position correction vector
        """
        out, _ = self.lstm(x)
        return self.fc(out[:, -1, :])


class Actor(nn.Module):
    """
    âœ… ê°œì„ : ë…¼ë¬¸ ëª…ì„¸ì— ë§ê²Œ ìˆ˜ì •
    - 1ê°œì˜ ì€ë‹‰ì¸µ (128 ë‰´ëŸ°)
    - Tanh í™œì„±í™” í•¨ìˆ˜
    - LSTM ë³€í˜•ì˜ ê²½ìš° LSTM ë ˆì´ì–´ ì¶”ê°€
    """
    def __init__(self, local_dim, act_dim, hidden=128, use_lstm=False):
        super().__init__()
        self.use_lstm = use_lstm
        self.fc1 = nn.Linear(local_dim, hidden)
        if use_lstm:
            self.lstm = nn.LSTM(hidden, hidden, batch_first=True)
        # âœ… ìˆ˜ì •: fc2 ì¸µ ì œê±° (ë…¼ë¬¸ì—ëŠ” 1ê°œ ì€ë‹‰ì¸µë§Œ)
        self.head = nn.Linear(hidden, act_dim)
    
    def forward(self, x):
        x = torch.tanh(self.fc1(x))
        if self.use_lstm:
            if x.dim() == 2:
                x = x.unsqueeze(1)
            x, _ = self.lstm(x)
            x = x[:, -1, :]
        # âœ… fc2 ì œê±°ë¡œ ì¸í•œ ìˆ˜ì •
        return F.softmax(self.head(x), dim=-1)


class Critic(nn.Module):
    """
    âœ… ë…¼ë¬¸ ëª…ì„¸ ì¤€ìˆ˜
    - 2ê°œì˜ ì€ë‹‰ì¸µ, ê° 256 ë‰´ëŸ°
    - Tanh í™œì„±í™” í•¨ìˆ˜
    """
    def __init__(self, glob_dim, hidden=256):
        super().__init__()
        self.net = nn.Sequential(
            nn.Linear(glob_dim, hidden), nn.Tanh(),
            nn.Linear(hidden, hidden), nn.Tanh(),
            nn.Linear(hidden, 1)
        )
    
    def forward(self, x):
        return self.net(x)


# ==================== ENVIRONMENT ====================

class EnvironmentScenario:
    """í™˜ê²½ ì‹œë‚˜ë¦¬ì˜¤ ìƒì„± (ì¥ì• ë¬¼, ì‹œì‘/ëª©í‘œ ìœ„ì¹˜)"""
    def __init__(self, config):
        self.config = config
        self.grid_size = config["grid_size"]
        self.num_uavs = config["num_uavs"]
        self.num_obstacles = config["num_obstacles"]
        self.grid = np.zeros((self.grid_size, self.grid_size), dtype=int)
        self._place_obstacles()
        self.start_positions, self.target_positions = self._generate_start_and_targets()
    
    def _place_obstacles(self):
        count = 0
        while count < self.num_obstacles:
            r = np.random.randint(0, self.grid_size)
            c = np.random.randint(0, self.grid_size)
            if self.grid[r, c] == 0:
                self.grid[r, c] = -1
                count += 1
    
    def _generate_start_and_targets(self):
        starts = []
        targets = []
        available_cells = [(r, c) for r in range(self.grid_size) for c in range(self.grid_size) if self.grid[r, c] == 0]
        chosen = np.random.choice(len(available_cells), 2 * self.num_uavs, replace=False)
        for i in range(self.num_uavs):
            starts.append(np.array(available_cells[chosen[2*i]], dtype=float))
            targets.append(np.array(available_cells[chosen[2*i+1]], dtype=float))
        return np.array(starts), np.array(targets)


class CTDEMultiUAVEnv:
    """
    CTDE (Centralized Training, Decentralized Execution) Multi-UAV í™˜ê²½
    
    âœ… ê°œì„ ì‚¬í•­:
    - ê´€ì°° ê³µê°„ì— ìœµí•©ëœ ìœ„ì¹˜ ì‚¬ìš©
    - ì†ë„ ì •ë³´ ì¶”ê°€
    - GPS ê³µê²© í™•ë¥  10%ë¡œ ìˆ˜ì •
    - Consensus Protocol íˆ¬í‘œ ë©”ì»¤ë‹ˆì¦˜ í†µí•©
    """
    def __init__(self, config, render_mode=None):
        self.config = config
        self.num_uavs = config["num_uavs"]
        self.grid_size = config["grid_size"]
        self.max_steps = config["max_steps"]
        self.vision_range = config["vision_range"]
        self.agents = [f"uav_{i}" for i in range(self.num_uavs)]
        
        # 8ë°©í–¥ ì´ë™ (ìƒí•˜ì¢Œìš° + ëŒ€ê°ì„ )
        self.discrete_moves = np.array([
            [0,-1], [0,1], [-1,0], [1,0],  # ìƒí•˜ì¢Œìš°
            [-1,-1], [-1,1], [1,-1], [1,1]  # ëŒ€ê°ì„ 
        ], dtype=int)
        
        # âœ… ê°œì„ : ê´€ì°° ê³µê°„ ì¬ì •ì˜ (ìœµí•©ëœ ìœ„ì¹˜ + ì†ë„ ì¶”ê°€)
        # ìì‹ ì˜ ìƒíƒœ: fused_pos(2) + velocity(2) + target(2) + trust_features(4) + consensus_vote(1) = 11
        self_dim = 2 + 2 + 2 + 4 + 1
        neighbor_dim = (self.num_uavs - 1) * 5  # ê° ì´ì›ƒ: rel_pos(2) + gps_pos(2) + discrepancy(1)
        vision_dim = (2 * self.vision_range + 1) ** 2
        self.local_obs_dim = self_dim + neighbor_dim + vision_dim
        
        self.global_obs_dim = (self.num_uavs * 4) + (self.grid_size * self.grid_size)
        self.action_dim = len(self.discrete_moves)
        
        self.render_mode = render_mode
        self.window = None
        if self.render_mode == "human":
            self._init_pygame()
        
        self.consensus = ConsensusProtocol(
            self.config["consensus_threshold"], 
            self.config["consensus_weight"],
            self.config["consensus_vote_threshold"]
        )
        
        self.attack_mode = config["attack_mode"]
        self.attack_start_prob = config.get("attack_start_prob", 0.1)  # âœ… ìˆ˜ì •: 10%
        self.attack_min_duration = config.get("attack_min_duration", 10)
        self.attack_max_duration = config.get("attack_max_duration", 30)
        
        # ê³µê²© ìƒíƒœ ê´€ë¦¬
        self.attack_remaining_steps = np.zeros(self.num_uavs, dtype=int)
        self.attack_drift_dir = np.zeros((self.num_uavs, 2), dtype=float)
        self.attack_step_offset = np.zeros((self.num_uavs, 2), dtype=float)
        self.active_attack_types = ["none"] * self.num_uavs
        
        self.consensus_votes = np.zeros(self.num_uavs, dtype=float)
        self.suspicion_votes_received = [[] for _ in range(self.num_uavs)]  # âœ… ì¶”ê°€
    
    def reset_with_scenario(self, scenario):
        """ì‹œë‚˜ë¦¬ì˜¤ë¡œ í™˜ê²½ ì´ˆê¸°í™”"""
        self.current_step = 0
        self.grid = scenario.grid.copy()
        self.shared_map = np.full((self.grid_size, self.grid_size), -2.0, dtype=np.float32)
        
        self.uav_positions = scenario.start_positions.copy().astype(float)
        self.target_positions = scenario.target_positions.copy()
        self.uav_status = ["active"] * self.num_uavs
        
        self.last_positions = self.uav_positions.copy()
        self.last_velocities = np.zeros((self.num_uavs, 2))
        self.gps_positions = self.uav_positions.copy()
        self.drift_bias = np.zeros((self.num_uavs, 2))
        self.is_under_attack = [False] * self.num_uavs
        
        # ê³µê²© ìƒíƒœ ì´ˆê¸°í™”
        self.attack_remaining_steps.fill(0)
        self.attack_drift_dir.fill(0)
        self.attack_step_offset.fill(0)
        self.active_attack_types = ["none"] * self.num_uavs
        
        self.prev_distances = np.linalg.norm(self.uav_positions - self.target_positions, axis=1)
        self.step_counts = [0] * self.num_uavs
        self.total_path_lengths = [0.0] * self.num_uavs
        self.uav_paths = {agent_id: [pos.copy()] for agent_id, pos in zip(self.agents, self.uav_positions)}
        
        self.suspicion_votes_received = [[] for _ in range(self.num_uavs)]  # âœ… ì¶”ê°€
        
        self._update_shared_map()
        return self._compute_observations()
    
    def step(self, actions):
        """í™˜ê²½ ìŠ¤í… ì‹¤í–‰"""
        self.current_step += 1
        self.last_positions = self.uav_positions.copy()
        
        # ê° UAV ì´ë™
        for i, aid in enumerate(self.agents):
            if self.uav_status[i] == "active":
                move = self.discrete_moves[actions[aid]]
                self.last_velocities[i] = move
                intended = self.uav_positions[i] + move
                self.total_path_lengths[i] += np.linalg.norm(move)
                
                # ì¶©ëŒ ì²´í¬
                is_collision = False
                if not (0 <= intended[0] < self.grid_size and 0 <= intended[1] < self.grid_size):
                    is_collision = True
                elif self.grid[int(intended[1]), int(intended[0])] == -1:
                    is_collision = True
                
                if is_collision:
                    self.uav_status[i] = "collision"
                else:
                    self.uav_positions[i] = intended
                    self.uav_paths[aid].append(intended.copy())
        
        # UAV ê°„ ì¶©ëŒ ì²´í¬
        for i in range(self.num_uavs):
            if self.uav_status[i] == "active":
                for j in range(i+1, self.num_uavs):
                    if self.uav_status[j] == "active":
                        if np.array_equal(self.uav_positions[i], self.uav_positions[j]):
                            self.uav_status[i] = "collision"
                            self.uav_status[j] = "collision"
        
        # ëª©í‘œ ë„ë‹¬ ì²´í¬
        for i in range(self.num_uavs):
            if self.uav_status[i] == "active":
                if np.linalg.norm(self.uav_positions[i] - self.target_positions[i]) < 1.5:
                    self.uav_status[i] = "success"
        
        # GPS ìŠ¤í‘¸í•‘ ê³µê²© ì‹œë®¬ë ˆì´ì…˜
        self._simulate_attacks()
        
        # ê³µìœ  ë§µ ì—…ë°ì´íŠ¸
        self._update_shared_map()
        
        # ë³´ìƒ ê³„ì‚°
        rewards = self._calculate_rewards()
        
        # ê´€ì°° ê³„ì‚°
        local, global_obs = self._compute_observations()
        
        # ì¢…ë£Œ ì¡°ê±´
        done = (all(s != "active" for s in self.uav_status)) or (self.current_step >= self.max_steps)
        info = self._get_info(done)
        
        return local, global_obs, rewards, done, info
    
    def _simulate_attacks(self):
        """
        GPS ìŠ¤í‘¸í•‘ ê³µê²© ì‹œë®¬ë ˆì´ì…˜
        
        ê³µê²© íƒ€ì…:
        1. Step Attack: ê³ ì • ì˜¤í”„ì…‹ (-4.0 ~ 4.0m)
        2. Drift Attack: ëˆ„ì  í¸í–¥ (0.2 ~ 0.8 m/s)
        3. Hybrid: ëœë¤ ì„ íƒ
        """
        self.gps_positions = self.uav_positions.copy()
        self.is_under_attack = [False] * self.num_uavs
        
        if self.config["attack_mode"] == "none":
            return
        
        for i in range(self.num_uavs):
            if self.uav_status[i] != "active":
                self.attack_remaining_steps[i] = 0
                continue
            
            # ì§„í–‰ ì¤‘ì¸ ê³µê²©
            if self.attack_remaining_steps[i] > 0:
                self.is_under_attack[i] = True
                atk_type = self.active_attack_types[i]
                
                if atk_type == "step":
                    self.gps_positions[i] += self.attack_step_offset[i]
                elif atk_type == "drift":
                    noise = np.random.normal(0.0, 0.1, size=2)
                    self.drift_bias[i] += self.attack_drift_dir[i] + noise
                    self.gps_positions[i] += self.drift_bias[i]
                
                self.attack_remaining_steps[i] -= 1
            
            # ìƒˆë¡œìš´ ê³µê²© ì‹œì‘
            elif np.random.rand() < self.attack_start_prob:
                duration = np.random.randint(self.attack_min_duration, self.attack_max_duration+1)
                self.attack_remaining_steps[i] = duration
                self.is_under_attack[i] = True
                
                # ê³µê²© íƒ€ì… ì„ íƒ
                if self.attack_mode == "hybrid":
                    self.active_attack_types[i] = "step" if np.random.rand() < 0.5 else "drift"
                else:
                    self.active_attack_types[i] = self.attack_mode
                
                # ê³µê²© íŒŒë¼ë¯¸í„° ì„¤ì •
                if self.active_attack_types[i] == "step":
                    self.attack_step_offset[i] = np.random.uniform(-4.0, 4.0, size=2)
                    self.gps_positions[i] += self.attack_step_offset[i]
                elif self.active_attack_types[i] == "drift":
                    angle = np.random.uniform(0, 2*np.pi)
                    self.attack_drift_dir[i] = np.array([np.cos(angle), np.sin(angle)]) * np.random.uniform(0.2, 0.8)
                    self.drift_bias[i] = np.zeros(2)
                    self.gps_positions[i] += self.drift_bias[i]
    
    def _compute_observations(self):
        """
        âœ… ê°œì„ : ë…¼ë¬¸ ëª…ì„¸ì— ë§ê²Œ ê´€ì°° ê³µê°„ ì¬êµ¬ì„±
        
        ê° UAVì˜ ê´€ì°°:
        - ìœµí•©ëœ ìœ„ì¹˜ (fused_pos) - ì•„ì§ ìœµí•©ë˜ì§€ ì•Šì€ ê²½ìš° GPS ì‚¬ìš©
        - ì†ë„ (velocity)
        - ëª©í‘œ ìœ„ì¹˜ (target)
        - Trust features (temporal_residual, spatial_discrepancy, gps_variance, neighbor_flag)
        - Consensus vote
        - ì´ì›ƒ ì •ë³´
        - Local vision
        """
        local_obs, all_states = {}, []
        
        # ê¸€ë¡œë²Œ ìƒíƒœ êµ¬ì„±
        for i in range(self.num_uavs):
            all_states.extend(self.uav_positions[i] / self.grid_size)
            all_states.extend(self.target_positions[i] / self.grid_size)
        
        # âœ… ì¶”ê°€: Consensus Protocol íˆ¬í‘œ ìˆ˜ì§‘
        self.suspicion_votes_received = [[] for _ in range(self.num_uavs)]
        
        # ê° UAVì˜ ë¡œì»¬ ê´€ì°° ìƒì„±
        for i in range(self.num_uavs):
            # Temporal Residual: ì˜ˆì¸¡ ìœ„ì¹˜ vs GPS ìœ„ì¹˜
            pred_pos = self.last_positions[i] + self.last_velocities[i]
            temp_res = np.linalg.norm(pred_pos - self.gps_positions[i])
            
            # GPS Variance (ê³µê²© ì—¬ë¶€ì— ë”°ë¼ ë‹¤ë¦„)
            gps_var = np.random.uniform(0.1, 0.5) if not self.is_under_attack[i] else np.random.uniform(2.0, 5.0)
            
            # ì´ì›ƒ ì •ë³´ ë° Spatial Discrepancy ê³„ì‚°
            neighbor_info, discrepancies = [], []
            for j in range(self.num_uavs):
                if i == j:
                    continue
                dist = np.linalg.norm(self.uav_positions[j] - self.uav_positions[i])
                if dist <= self.vision_range:
                    vis_pos = self.uav_positions[j]
                    gps_claim = self.gps_positions[j]
                    disc = np.linalg.norm(vis_pos - gps_claim)
                    discrepancies.append(disc)
                    
                    # âœ… ì¶”ê°€: íˆ¬í‘œ ìˆ˜í–‰
                    if disc > self.consensus.threshold:
                        self.suspicion_votes_received[j].append(1)  # jì—ê²Œ ì˜ì‹¬ í‘œ ì „ë‹¬
                    else:
                        self.suspicion_votes_received[j].append(0)
                    
                    neighbor_info.extend([
                        (vis_pos - self.uav_positions[i])/self.grid_size,  # ìƒëŒ€ ìœ„ì¹˜
                        (self.gps_positions[j] - self.gps_positions[i])/self.grid_size,  # GPS ìƒëŒ€ ìœ„ì¹˜
                        disc  # ë¶ˆì¼ì¹˜
                    ])
                else:
                    neighbor_info.extend([np.zeros(2), np.zeros(2), 0.0])
            
            # Flatten neighbor info
            flat_neighbor = []
            for item in neighbor_info:
                if isinstance(item, np.ndarray):
                    flat_neighbor.extend(item)
                else:
                    flat_neighbor.append(item)
            neighbor_info = np.array(flat_neighbor, dtype=np.float32)
            
            # Consensus Vote ê³„ì‚°
            spat_disc = np.mean(discrepancies) if discrepancies else 0.0
            self.consensus_votes[i] = spat_disc
            
            # Trust Features (ì •ê·œí™”)
            norm_temp = np.clip(temp_res / 2.0, 0.0, 1.0)
            norm_spat = np.clip(spat_disc / 1.0, 0.0, 1.0)
            
            trust_feats = np.array([
                norm_temp,
                norm_spat,
                gps_var,
                1.0 if discrepancies else 0.0  # ì´ì›ƒ ì¡´ì¬ ì—¬ë¶€
            ], dtype=np.float32)
            
            # âœ… ê°œì„ : ìœµí•©ëœ ìœ„ì¹˜ ì‚¬ìš© (í˜„ì¬ëŠ” GPS, Agentì—ì„œ ìœµí•© í›„ ì—…ë°ì´íŠ¸)
            # ì´ˆê¸° ìƒíƒœì—ì„œëŠ” GPS ìœ„ì¹˜ ì‚¬ìš©
            my_state = np.concatenate([
                self.gps_positions[i] / self.grid_size,  # fused_pos (ë‚˜ì¤‘ì— Agentì—ì„œ ì—…ë°ì´íŠ¸)
                self.last_velocities[i] / self.grid_size,  # âœ… ì¶”ê°€: velocity
                self.target_positions[i] / self.grid_size,
                trust_feats,
                [spat_disc]  # consensus vote
            ])
            
            local_vis = self._extract_local_vision(self.uav_positions[i])
            local_obs[self.agents[i]] = np.concatenate([my_state, neighbor_info, local_vis]).astype(np.float32)
        
        global_obs = np.concatenate([np.array(all_states), self.shared_map.flatten()]).astype(np.float32)
        return local_obs, global_obs
    
    def _extract_local_vision(self, pos):
        """ë¡œì»¬ Vision ì„¼ì„œ (ì£¼ë³€ ì¥ì• ë¬¼ ê´€ì¸¡)"""
        r = self.vision_range
        roi = np.full((2*r+1, 2*r+1), -2.0, dtype=np.float32)
        px, py = int(pos[0]), int(pos[1])
        for dy in range(-r, r+1):
            for dx in range(-r, r+1):
                ny, nx = py + dy, px + dx
                if 0 <= ny < self.grid_size and 0 <= nx < self.grid_size:
                    roi[dy+r, dx+r] = self.grid[ny, nx]
        return roi.flatten()
    
    def _update_shared_map(self):
        """ê³µìœ  ë§µ ì—…ë°ì´íŠ¸ (ê° UAVì˜ Vision ë²”ìœ„ ë‚´ ì •ë³´ ê³µìœ )"""
        for i in range(self.num_uavs):
            if self.uav_status[i] == 'active':
                px, py = int(self.uav_positions[i][0]), int(self.uav_positions[i][1])
                r = self.vision_range
                y1, y2 = max(0, py-r), min(self.grid_size, py+r+1)
                x1, x2 = max(0, px-r), min(self.grid_size, px+r+1)
                self.shared_map[y1:y2, x1:x2] = self.grid[y1:y2, x1:x2]
    
    def _calculate_rewards(self):
        """ë³´ìƒ ê³„ì‚°"""
        rewards = {}
        for i, aid in enumerate(self.agents):
            r = self.config["reward_step_penalty"]
            
            if self.uav_status[i] == "success":
                r += self.config["reward_goal"]
            elif self.uav_status[i] == "collision":
                r += self.config["reward_collision"]
            else:
                # ëª©í‘œ ì ‘ê·¼ ë³´ìƒ
                dist = np.linalg.norm(self.uav_positions[i] - self.target_positions[i])
                r += (self.prev_distances[i] - dist) * self.config["distance_reward_factor"] * 10.0
            
            rewards[aid] = r
        
        self.prev_distances = np.linalg.norm(self.uav_positions - self.target_positions, axis=1)
        return rewards
    
    def _get_info(self, done):
        """ì—í”¼ì†Œë“œ ì¢…ë£Œ ì •ë³´"""
        if not done:
            return {}
        
        s = sum(1 for st in self.uav_status if st == "success")
        c = sum(1 for st in self.uav_status if st == "collision")
        
        return {
            "success_rate": s / self.num_uavs,
            "collision_rate": c / self.num_uavs,
            "avg_path_length": np.mean(self.total_path_lengths)
        }
    
    def _init_pygame(self):
        """Pygame ì´ˆê¸°í™” (ì‹œê°í™”ìš©)"""
        pygame.init()
        self.window_size = 600
        self.cell_size = self.window_size // self.grid_size
        self.window = pygame.display.set_mode((self.window_size, self.window_size))
        pygame.display.set_caption("Multi-UAV Navigation (Improved)")
        self.uav_colors = [(255,0,0), (0,255,0), (0,0,255), (255,255,0), (255,0,255)]
        self.clock = pygame.time.Clock()
    
    def render(self):
        """í™˜ê²½ ë Œë”ë§"""
        if self.render_mode != "human":
            return
        
        self.window.fill((255, 255, 255))
        
        # ì¥ì• ë¬¼ ê·¸ë¦¬ê¸°
        for r in range(self.grid_size):
            for c in range(self.grid_size):
                if self.grid[r, c] == -1:
                    pygame.draw.rect(self.window, (50, 50, 50), 
                                   (c*self.cell_size, r*self.cell_size, 
                                    self.cell_size, self.cell_size))
        
        # UAV ê²½ë¡œ ë° ìœ„ì¹˜ ê·¸ë¦¬ê¸°
        for i in range(self.num_uavs):
            color = self.uav_colors[i % len(self.uav_colors)]
            
            # ê²½ë¡œ
            if len(self.uav_paths[self.agents[i]]) > 1:
                pts = [(p[0]*self.cell_size+self.cell_size/2, 
                       p[1]*self.cell_size+self.cell_size/2) 
                       for p in self.uav_paths[self.agents[i]]]
                pygame.draw.lines(self.window, color, False, pts, 2)
            
            # UAV ìœ„ì¹˜
            pygame.draw.circle(self.window, color, 
                             (int(self.uav_positions[i][0]*self.cell_size+self.cell_size/2),
                              int(self.uav_positions[i][1]*self.cell_size+self.cell_size/2)), 5)
        
        pygame.display.flip()
        self.clock.tick(10)
    
    def close(self):
        """í™˜ê²½ ì¢…ë£Œ"""
        if self.window:
            pygame.quit()
            self.window = None


# ==================== ROLLOUT BUFFER ====================

class RolloutBuffer:
    """ê²½í—˜ ì €ì¥ ë²„í¼"""
    def __init__(self):
        self.clear()
    
    def clear(self):
        self.obs = []
        self.glo = []
        self.act = []
        self.logp = []
        self.val = []
        self.rew = []
        self.done = []
        self.adv = []
        self.ret = []
    
    def add(self, o, g, a, l, v, r, d):
        self.obs.extend(o)
        self.glo.extend(g)
        self.act.extend(a)
        self.logp.extend(l)
        self.val.extend(v)
        self.rew.extend(r)
        self.done.extend(d)


# ==================== AGENT ====================

class MAPPOAgentWithTrust:
    """
    âœ… ê°œì„ : ë…¼ë¬¸ ëª…ì„¸ì— ë§ê²Œ Trust Network í†µí•© MAPPO Agent
    
    ì£¼ìš” ê°œì„ ì‚¬í•­:
    1. Trust Network Learning Rateë¥¼ Actorì˜ 50%ë¡œ ì„¤ì •
    2. Consensus Protocol 50% íˆ¬í‘œ ë©”ì»¤ë‹ˆì¦˜ í†µí•©
    3. ìœµí•©ëœ ìœ„ì¹˜ë¥¼ Actor ì…ë ¥ìœ¼ë¡œ ì‚¬ìš©
    4. Trust Loss lambdaë¥¼ 0.1ë¡œ ìˆ˜ì •
    """
    def __init__(self, l_dim, g_dim, a_dim, config):
        self.config = config
        self.device = DEVICE
        
        # Actor & Critic ë„¤íŠ¸ì›Œí¬
        self.actor = Actor(l_dim, a_dim, hidden=128, use_lstm=config.get("use_lstm_detection", False)).to(DEVICE)
        self.critic = Critic(g_dim, hidden=256).to(DEVICE)
        
        # âœ… ìˆ˜ì •: ë…¼ë¬¸ ëª…ì„¸ì— ë§ëŠ” Learning Rate
        self.actor_opt = optim.Adam(self.actor.parameters(), lr=config["mappo_lr"])
        self.critic_opt = optim.Adam(self.critic.parameters(), lr=config["mappo_lr"])
        
        # Trust Network
        self.use_trust = config.get("use_trust_network", False)
        self.use_consensus = config.get("use_consensus", False)
        self.use_detector = config.get("use_spoof_lstm_detector", False)
        
        if self.use_trust:
            self.trust_net = TrustNetwork(config["trust_hidden"]).to(DEVICE)
            # âœ… ìˆ˜ì •: Trust Network LR = Actor LR * 50%
            self.trust_opt = optim.Adam(self.trust_net.parameters(), lr=config["trust_lr"])
            self.trust_loss = TrustLoss(config["trust_lambda_reg"])
            self.last_trust_scores = {}
        
        if self.use_consensus:
            self.consensus = ConsensusProtocol(
                config["consensus_threshold"], 
                config["consensus_weight"],
                config["consensus_vote_threshold"]
            )
        
        if self.use_detector:
            self.detector = LSTMSpoofDetector(
                config["detector_feature_dim"], 
                config["detector_hidden"]
            ).to(DEVICE)
            self.det_opt = optim.Adam(self.detector.parameters(), lr=config["mappo_lr"])
            self.det_hist = {}
            self.det_buf = {"in": [], "tgt": []}
        
        self.buffer = RolloutBuffer()
        self.trust_buf = {"feat": [], "gps": [], "real": [], "prev": []}  # âœ… ìˆ˜ì •: gps ì¶”ê°€, fused/curr ì œê±°
    
    def reset_episode(self, agents):
        """ì—í”¼ì†Œë“œ ì‹œì‘ ì‹œ ì´ˆê¸°í™”"""
        if self.use_trust:
            self.last_trust_scores = {a: torch.tensor([0.5, 0.5], device=DEVICE) for a in agents}
        if self.use_detector:
            self.det_hist = {a: deque(maxlen=self.config["detector_seq_len"]) for a in agents}
    
    def select_action(self, l_obs, g_obs, real_pos, gps_pos, env=None, deterministic=False):
        """
        âœ… ê°œì„ : ì•¡ì…˜ ì„ íƒ ì‹œ ìœµí•©ëœ ìœ„ì¹˜ ì‚¬ìš© ë° Consensus íˆ¬í‘œ ë©”ì»¤ë‹ˆì¦˜ ì ìš©
        
        Args:
            l_obs: ë¡œì»¬ ê´€ì°° ë”•ì…”ë„ˆë¦¬
            g_obs: ê¸€ë¡œë²Œ ê´€ì°°
            real_pos: ì‹¤ì œ ìœ„ì¹˜ (í•™ìŠµìš©)
            gps_pos: GPS ìœ„ì¹˜
            env: í™˜ê²½ ê°ì²´ (Consensus íˆ¬í‘œìš©)
            deterministic: ê²°ì •ì  ì•¡ì…˜ ì„ íƒ ì—¬ë¶€
        
        Returns:
            actions, log_probs, value, trust_info
        """
        with torch.no_grad():
            actions, log_probs, trust_info = {}, {}, {}
            g_tensor = torch.tensor(g_obs, dtype=torch.float32, device=DEVICE).unsqueeze(0)
            val = self.critic(g_tensor).item()
            
            for aid, obs in l_obs.items():
                idx = int(aid.split('_')[1])
                obs_t = torch.tensor(obs, dtype=torch.float32, device=DEVICE).unsqueeze(0)
                obs_mod = obs.copy()
                t_gps, t_vis = 1.0, 0.0
                fused_pos_np = gps_pos[idx].copy()
                
                if self.use_trust:
                    # Trust Networkë¡œ ì‹ ë¢°ë„ ê³„ì‚°
                    t_feat = obs_t[:, 6:10]  # trust_features (4ì°¨ì›)
                    t_out = self.trust_net(t_feat).squeeze(0)
                    
                    # Consensus Vote (ê´€ì°° ê³µê°„ì˜ ë§ˆì§€ë§‰ trust feature ë‹¤ìŒ)
                    vote = obs[10] if self.use_consensus else 0.0
                    
                    # âœ… ê°œì„ : Consensus Protocol ì ìš©
                    force_zero = False
                    if self.use_consensus and env is not None:
                        # ë°›ì€ ì˜ì‹¬ í‘œ ì§‘ê³„
                        votes_received = env.suspicion_votes_received[idx]
                        is_under_attack, suspicion_ratio = self.consensus.aggregate_votes(votes_received)
                        force_zero = is_under_attack
                        
                        # Trust ì¡°ì •
                        t_gps, t_vis = self.consensus.adjust_trust(
                            t_out[0].item(), 
                            t_out[1].item(), 
                            vote,
                            force_zero=force_zero
                        )
                    else:
                        t_gps, t_vis = t_out[0].item(), t_out[1].item()
                    
                    trust_info[aid] = {
                        'gps': t_gps, 
                        'vis': t_vis,
                        'force_zero': force_zero
                    }
                    
                    # âœ… ê°œì„ : ìœµí•©ëœ ìœ„ì¹˜ ê³„ì‚°
                    if real_pos is not None:
                        gp = torch.tensor(gps_pos[idx], device=DEVICE, dtype=torch.float32)
                        rp = torch.tensor(real_pos[idx], device=DEVICE, dtype=torch.float32)
                        
                        # âœ… ìˆ˜ì •: gradientë¥¼ ìœ ì§€í•˜ê¸° ìœ„í•´ t_out í…ì„œ ì§ì ‘ ì‚¬ìš©
                        fused = t_out[0] * gp + t_out[1] * rp
                        prev = self.last_trust_scores.get(aid, torch.tensor([0.5, 0.5], device=DEVICE))
                        
                        # Trust Loss ê³„ì‚°ìš© ë²„í¼ì— ì €ì¥
                        self.trust_buf['feat'].append(t_feat.squeeze(0))  # (4,)
                        self.trust_buf['gps'].append(gp)  # âœ… ì¶”ê°€: GPS ìœ„ì¹˜ ì €ì¥
                        self.trust_buf['real'].append(rp)
                        self.trust_buf['prev'].append(prev)
                        self.last_trust_scores[aid] = t_out.detach()
                        
                        fused_pos_np = fused.detach().cpu().numpy()
                        
                        fused_pos_np = fused.cpu().numpy()
                    else:
                        # ì‹¤ì œ ìœ„ì¹˜ê°€ ì—†ëŠ” ê²½ìš° (í‰ê°€ ëª¨ë“œ) GPSì™€ ê´€ì¸¡ ìœ„ì¹˜ ìœµí•©
                        fused_pos_np = t_gps * gps_pos[idx] + t_vis * gps_pos[idx]  # ê·¼ì‚¬ì¹˜
                    
                    # âœ… ê°œì„ : Actor ì…ë ¥ì— ìœµí•©ëœ ìœ„ì¹˜ ì‚¬ìš©
                    obs_mod[0:2] = fused_pos_np / self.config["grid_size"]
                    obs_t = torch.tensor(obs_mod, dtype=torch.float32, device=DEVICE).unsqueeze(0)
                
                elif self.use_detector:
                    # LSTM Detector ì‚¬ìš©
                    gps_norm = gps_pos[idx] / self.config["grid_size"]
                    feat = np.array([gps_norm[0], gps_norm[1], obs[6], obs[7], obs[10]], dtype=np.float32)
                    self.det_hist[aid].append(feat)
                    seq = list(self.det_hist[aid])
                    while len(seq) < self.config["detector_seq_len"]:
                        seq.insert(0, [0]*5)
                    
                    seq_t = torch.tensor(seq, dtype=torch.float32, device=DEVICE).unsqueeze(0)
                    correction = self.detector(seq_t).squeeze(0).cpu().numpy()
                    obs_mod[0:2] = gps_norm + correction
                    obs_t = torch.tensor(obs_mod, dtype=torch.float32, device=DEVICE).unsqueeze(0)
                    
                    if real_pos is not None:
                        tgt = (real_pos[idx] / self.config["grid_size"]) - gps_norm
                        self.det_buf["in"].append(seq)
                        self.det_buf["tgt"].append(tgt)
                    
                    trust_info[aid] = {'gps': 1.0, 'vis': 0.0}
                else:
                    # Trust Network ë¯¸ì‚¬ìš© (Baseline)
                    trust_info[aid] = {'gps': 1.0, 'vis': 0.0}
                
                # Actorë¡œ ì•¡ì…˜ ì„ íƒ
                probs = self.actor(obs_t)
                dist = Categorical(probs)
                act = torch.argmax(probs) if deterministic else dist.sample()
                actions[aid] = act.item()
                log_probs[aid] = dist.log_prob(act).item()
            
            return actions, log_probs, val, trust_info
    
    def compute_gae(self, next_val):
        """Generalized Advantage Estimation (GAE) ê³„ì‚°"""
        rews = torch.tensor(self.buffer.rew, dtype=torch.float32)
        vals = torch.tensor(self.buffer.val + [next_val], dtype=torch.float32)
        dones = torch.tensor(self.buffer.done, dtype=torch.float32)
        
        adv = []
        last = 0
        for t in reversed(range(len(rews))):
            delta = rews[t] + self.config["gamma"] * vals[t+1] * (1-dones[t]) - vals[t]
            last = delta + self.config["gamma"] * self.config["gae_lambda"] * (1-dones[t]) * last
            adv.insert(0, last)
        
        self.buffer.adv = adv
        self.buffer.ret = [a + v for a, v in zip(adv, vals[:-1].tolist())]
    
    def update(self):
        """PPO ì—…ë°ì´íŠ¸"""
        b_obs = torch.tensor(np.array(self.buffer.obs), dtype=torch.float32, device=DEVICE)
        b_glo = torch.tensor(np.array(self.buffer.glo), dtype=torch.float32, device=DEVICE)
        b_act = torch.tensor(self.buffer.act, dtype=torch.long, device=DEVICE)
        b_log = torch.tensor(self.buffer.logp, dtype=torch.float32, device=DEVICE)
        b_adv = torch.tensor(self.buffer.adv, dtype=torch.float32, device=DEVICE)
        b_ret = torch.tensor(self.buffer.ret, dtype=torch.float32, device=DEVICE)
        
        # PPO Update
        for _ in range(self.config["update_epochs"]):
            probs = self.actor(b_obs)
            dist = Categorical(probs)
            log_p = dist.log_prob(b_act)
            ratio = torch.exp(log_p - b_log)
            
            surr1 = ratio * b_adv
            surr2 = torch.clamp(ratio, 0.8, 1.2) * b_adv
            a_loss = -torch.min(surr1, surr2).mean()
            
            c_loss = F.mse_loss(self.critic(b_glo).squeeze(), b_ret)
            loss = a_loss + 0.5 * c_loss - self.config["mappo_entropy"] * dist.entropy().mean()
            
            self.actor_opt.zero_grad()
            self.critic_opt.zero_grad()
            loss.backward()
            self.actor_opt.step()
            self.critic_opt.step()
        
        # Trust Network Update
        if self.use_trust and self.trust_buf['feat']:
            # âœ… ìˆ˜ì •: Trust Networkë¥¼ ë‹¤ì‹œ forward pass í•˜ì—¬ gradient ì—°ê²°
            feat_tensor = torch.stack(self.trust_buf['feat'])  # (N, 4)
            gps_tensor = torch.stack(self.trust_buf['gps'])    # (N, 2)
            real_tensor = torch.stack(self.trust_buf['real'])  # (N, 2)
            prev_tensor = torch.stack(self.trust_buf['prev'])  # (N, 2)
            
            # Trust Network forward (gradient í™œì„±í™”)
            trust_out = self.trust_net(feat_tensor)  # (N, 2) [GPS_trust, Vision_trust]
            
            # ìœµí•©ëœ ìœ„ì¹˜ ê³„ì‚°
            fused_pos = trust_out[:, 0:1] * gps_tensor + trust_out[:, 1:2] * real_tensor
            
            # Loss ê³„ì‚°: Fusion Loss + Smoothness Loss
            fusion_loss = torch.mean((fused_pos - real_tensor) ** 2)
            smoothness_loss = torch.mean((trust_out - prev_tensor) ** 2)
            loss = fusion_loss + self.trust_loss.lambda_reg * smoothness_loss
            
            self.trust_opt.zero_grad()
            loss.backward()
            self.trust_opt.step()
            
            self.trust_buf = {k: [] for k in self.trust_buf}
        
        # LSTM Detector Update
        if self.use_detector and self.det_buf['in']:
            inp = torch.tensor(np.array(self.det_buf['in']), dtype=torch.float32, device=DEVICE)
            tgt = torch.tensor(np.array(self.det_buf['tgt']), dtype=torch.float32, device=DEVICE)
            loss = F.mse_loss(self.detector(inp), tgt)
            
            self.det_opt.zero_grad()
            loss.backward()
            self.det_opt.step()
            
            self.det_buf = {"in": [], "tgt": []}
        
        self.buffer.clear()
    
    def save_models(self, path):
        """ëª¨ë¸ ì €ì¥"""
        os.makedirs(path, exist_ok=True)
        torch.save(self.actor.state_dict(), os.path.join(path, "actor.pth"))
        torch.save(self.critic.state_dict(), os.path.join(path, "critic.pth"))
        if self.use_trust:
            torch.save(self.trust_net.state_dict(), os.path.join(path, "trust.pth"))
        if self.use_detector:
            torch.save(self.detector.state_dict(), os.path.join(path, "detector.pth"))
    
    def load_models(self, path):
        """ëª¨ë¸ ë¡œë“œ"""
        self.actor.load_state_dict(torch.load(os.path.join(path, "actor.pth"), map_location=DEVICE))
        self.critic.load_state_dict(torch.load(os.path.join(path, "critic.pth"), map_location=DEVICE))
        if self.use_trust and os.path.exists(os.path.join(path, "trust.pth")):
            self.trust_net.load_state_dict(torch.load(os.path.join(path, "trust.pth"), map_location=DEVICE))
        if self.use_detector and os.path.exists(os.path.join(path, "detector.pth")):
            self.detector.load_state_dict(torch.load(os.path.join(path, "detector.pth"), map_location=DEVICE))


# ==================== TRAINING ====================

class TrainingWorker(threading.Thread):
    """í•™ìŠµ ì›Œì»¤ ìŠ¤ë ˆë“œ"""
    def __init__(self, config, algorithm_name, data_queue, stop_flag):
        super().__init__()
        self.config = config
        self.algorithm_name = algorithm_name
        self.data_queue = data_queue
        self.stop_flag = stop_flag
    
    def run(self):
        run_training(self.config, self.algorithm_name, self.data_queue, self.stop_flag)


def run_training(config, algorithm_name, data_queue, stop_flag):
    """
    í•™ìŠµ ì‹¤í–‰ í•¨ìˆ˜
    
    âœ… ê°œì„ : select_action í˜¸ì¶œ ì‹œ env ê°ì²´ ì „ë‹¬
    """
    try:
        np.random.seed(config['seed'])
        torch.manual_seed(config['seed'])
        
        base_folder = create_model_folder_name(config, algorithm_name)
        model_base_path = os.path.join("./models", base_folder)
        os.makedirs(model_base_path, exist_ok=True)
        writer = SummaryWriter(os.path.join("runs", base_folder))
        
        data_queue.put(("log", f"ğŸ”¥ {algorithm_name} í•™ìŠµ ì‹œì‘\n"))
        env = CTDEMultiUAVEnv(config)
        agent = MAPPOAgentWithTrust(env.local_obs_dim, env.global_obs_dim, env.action_dim, config)
        
        for ep in range(0, config["total_episodes"], config["episodes_per_batch"]):
            if stop_flag[0]:
                break
            
            rew_list, succ_list, coll_list = [], [], []
            
            for _ in range(config["episodes_per_batch"]):
                scen = EnvironmentScenario(config)
                lo, go = env.reset_with_scenario(scen)
                agent.reset_episode(env.agents)
                done = False
                ep_r = 0
                
                # ì—í”¼ì†Œë“œ ë²„í¼
                ep_obs, ep_glo, ep_act, ep_logp, ep_val, ep_rew, ep_done = [],[],[],[],[],[],[]
                
                while not done:
                    # âœ… ê°œì„ : env ê°ì²´ë¥¼ select_actionì— ì „ë‹¬
                    acts, logs, val, trust_info = agent.select_action(
                        lo, go, env.uav_positions, env.gps_positions, env=env
                    )
                    n_lo, n_go, rew, done, info = env.step(acts)
                    
                    ep_obs.extend([lo[a] for a in env.agents if a in acts])
                    ep_glo.extend([go for _ in acts])
                    ep_act.extend(list(acts.values()))
                    ep_logp.extend(list(logs.values()))
                    ep_val.extend([val for _ in acts])
                    ep_rew.extend(list(rew.values()))
                    ep_done.extend([done for _ in acts])
                    
                    lo, go = n_lo, n_go
                    ep_r += sum(rew.values())
                
                # ë²„í¼ì— ì¶”ê°€
                agent.buffer.add(ep_obs, ep_glo, ep_act, ep_logp, ep_val, ep_rew, ep_done)
                
                rew_list.append(ep_r)
                succ_list.append(info.get("success_rate", 0))
                coll_list.append(info.get("collision_rate", 0))
            
            # GAE ê³„ì‚° & ì—…ë°ì´íŠ¸
            with torch.no_grad():
                next_val = agent.critic(torch.tensor(go, dtype=torch.float32, device=DEVICE).unsqueeze(0)).item()
            agent.compute_gae(next_val)
            agent.update()
            
            # ë¡œê·¸
            avg_r, avg_s, avg_c = np.mean(rew_list), np.mean(succ_list), np.mean(coll_list)
            writer.add_scalar("Reward", avg_r, ep)
            writer.add_scalar("Success", avg_s, ep)
            writer.add_scalar("Collision", avg_c, ep)
            
            if ep % 100 == 0:
                data_queue.put(("log", f"[{algorithm_name}] Ep {ep}: Rew {avg_r:.1f} Succ {avg_s:.1%} Coll {avg_c:.1%}\n"))
                data_queue.put(("graph", {
                    "algorithm": algorithm_name,
                    "rew": avg_r,
                    "succ": avg_s,
                    "coll": avg_c,
                    "drift_det": 0,
                    "path_len": 0
                }))
            
            if ep % config["checkpoint_interval"] == 0 and ep > 0:
                agent.save_models(os.path.join(model_base_path, f"ckpt_{ep}"))
        
        agent.save_models(os.path.join(model_base_path, "final"))
        data_queue.put(("log", f"âœ… [{algorithm_name}] í•™ìŠµ ì™„ë£Œ\n"))
        data_queue.put(("done", algorithm_name))
        
    except Exception as e:
        import traceback
        data_queue.put(("log", f"âŒ Error in {algorithm_name}: {e}\n{traceback.format_exc()}\n"))
    finally:
        writer.close()


# ==================== GUI ====================

class GraphCanvas(FigureCanvas):
    """í•™ìŠµ ì§„í–‰ ê·¸ë˜í”„"""
    def __init__(self, parent=None):
        self.fig = Figure()
        super().__init__(self.fig)
        self.ax = self.fig.add_subplot(111)
        self.ax.set_title("Training Progress: Reward / Success / Collision")
        self.ax.set_xlabel("Episode")
        self.ax.set_ylabel("Value")
        self.graph_data = {}
    
    def update_graph(self, algorithm, rew, succ, coll, drift_det, path_len):
        if algorithm not in self.graph_data:
            self.graph_data[algorithm] = {'x': [], 'rew': [], 'succ': []}
        
        d = self.graph_data[algorithm]
        x = len(d['x']) * 100
        d['x'].append(x)
        d['rew'].append(rew)
        d['succ'].append(succ)
        
        self.ax.clear()
        for algo, vals in self.graph_data.items():
            self.ax.plot(vals['x'], vals['rew'], label=f"{algo} (Reward)", marker='o')
            # Success rateëŠ” ë³´ì¡° ì¶•ì— í‘œì‹œ (ì„ íƒì )
        
        self.ax.legend()
        self.ax.grid(True, alpha=0.3)
        self.draw()


class MainWindow(QMainWindow):
    """ë©”ì¸ GUI ìœˆë„ìš°"""
    def __init__(self):
        super().__init__()
        self.setWindowTitle("ğŸš Trust-Consensus MAPPO - Improved (ë…¼ë¬¸ ëª…ì„¸ ì¤€ìˆ˜)")
        self.setGeometry(100, 100, 1400, 900)
        self.data_queue = queue.Queue()
        self.stop_flag = [False]
        self.running_threads = {}
        
        main_widget = QWidget()
        self.setCentralWidget(main_widget)
        main_layout = QHBoxLayout(main_widget)
        
        # ì™¼ìª½ íŒ¨ë„: ì„¤ì • ë° ì œì–´
        left_panel = QVBoxLayout()
        title = QLabel("ğŸ¯ ì‹¤í—˜ ì„¤ì • ë° ì œì–´ (ë…¼ë¬¸ ëª…ì„¸ ë²„ì „)")
        title.setFont(QFont("Arial", 14, QFont.Weight.Bold))
        left_panel.addWidget(title)
        
        # ì•Œê³ ë¦¬ì¦˜ ì„ íƒ
        algo_group = QGroupBox("ì•Œê³ ë¦¬ì¦˜ ì„ íƒ")
        algo_layout = QVBoxLayout()
        self.algo_checkboxes = {}
        for algo_name, algo_config in ALGORITHM_CONFIGS.items():
            cb = QCheckBox(f"{algo_name}: {algo_config['description']}")
            self.algo_checkboxes[algo_name] = cb
            algo_layout.addWidget(cb)
        algo_group.setLayout(algo_layout)
        left_panel.addWidget(algo_group)
        
        # ê³µê²© ëª¨ë“œ ì„ íƒ
        attack_group = QGroupBox("ê³µê²© ëª¨ë“œ")
        attack_layout = QHBoxLayout()
        self.attack_combo = QComboBox()
        self.attack_combo.addItems(["hybrid", "step", "drift", "none"])
        attack_layout.addWidget(QLabel("Attack Mode:"))
        attack_layout.addWidget(self.attack_combo)
        attack_group.setLayout(attack_layout)
        left_panel.addWidget(attack_group)
        
        # í•™ìŠµ ì„¤ì •
        config_group = QGroupBox("í•™ìŠµ ì„¤ì •")
        config_layout = QFormLayout()
        self.episode_input = QLineEdit(str(BASE_CONFIG["total_episodes"]))
        self.batch_input = QLineEdit(str(BASE_CONFIG["episodes_per_batch"]))
        self.obstacle_input = QLineEdit(str(BASE_CONFIG["num_obstacles"]))
        config_layout.addRow("ì´ Episodes:", self.episode_input)
        config_layout.addRow("Batch Episodes:", self.batch_input)
        config_layout.addRow("ì¥ì• ë¬¼ ìˆ˜:", self.obstacle_input)
        config_group.setLayout(config_layout)
        left_panel.addWidget(config_group)
        
        # ë²„íŠ¼
        btn_layout1 = QHBoxLayout()
        self.start_btn = QPushButton("ğŸš€ í•™ìŠµ ì‹œì‘")
        self.start_btn.clicked.connect(self.start_training)
        self.stop_btn = QPushButton("â¹ï¸ ì¤‘ë‹¨")
        self.stop_btn.clicked.connect(self.stop_all_training)
        
        btn_layout1.addWidget(self.start_btn)
        btn_layout1.addWidget(self.stop_btn)
        left_panel.addLayout(btn_layout1)
        
        # ë°ëª¨ ë° ë„êµ¬ ë²„íŠ¼
        btn_layout2 = QHBoxLayout()
        self.demo_btn = QPushButton("ğŸ® ë°ëª¨ ì‹¤í–‰")
        self.demo_btn.clicked.connect(self.run_demo)
        self.tb_btn = QPushButton("ğŸ“Š TensorBoard")
        self.tb_btn.clicked.connect(self.open_tensorboard)
        
        btn_layout2.addWidget(self.demo_btn)
        btn_layout2.addWidget(self.tb_btn)
        left_panel.addLayout(btn_layout2)
        
        left_panel.addStretch()
        main_layout.addLayout(left_panel, 1)
        
        # ì˜¤ë¥¸ìª½ íŒ¨ë„: ê·¸ë˜í”„ ë° ë¡œê·¸
        right_panel = QVBoxLayout()
        self.graph_canvas = GraphCanvas(self)
        right_panel.addWidget(self.graph_canvas, 2)
        
        self.log_box = QTextEdit()
        self.log_box.setReadOnly(True)
        self.log_box.setFont(QFont("Consolas", 9))
        right_panel.addWidget(self.log_box, 1)
        
        main_layout.addLayout(right_panel, 2)
        
        # íƒ€ì´ë¨¸ (í ì²˜ë¦¬)
        self.timer = QTimer()
        self.timer.timeout.connect(self.process_queue)
        self.timer.start(200)
    
    def append_log(self, text):
        """ë¡œê·¸ ì¶”ê°€"""
        self.log_box.moveCursor(QTextCursor.MoveOperation.End)
        self.log_box.insertPlainText(text)
        self.log_box.moveCursor(QTextCursor.MoveOperation.End)
    
    def process_queue(self):
        """ë°ì´í„° í ì²˜ë¦¬"""
        while not self.data_queue.empty():
            item_type, payload = self.data_queue.get()
            if item_type == "log":
                self.append_log(payload)
            elif item_type == "graph":
                algo = payload['algorithm']
                self.graph_canvas.update_graph(
                    algo,
                    payload['rew'],
                    payload['succ'],
                    payload['coll'],
                    payload['drift_det'],
                    payload['path_len']
                )
    
    def start_training(self):
        """í•™ìŠµ ì‹œì‘"""
        self.stop_flag[0] = False
        selected_algos = [name for name, cb in self.algo_checkboxes.items() if cb.isChecked()]
        
        if not selected_algos:
            self.append_log("âš ï¸ ì•Œê³ ë¦¬ì¦˜ì„ ì„ íƒí•´ì£¼ì„¸ìš”.\n")
            return
        
        total_ep = int(self.episode_input.text())
        batch_ep = int(self.batch_input.text())
        obs_num = int(self.obstacle_input.text())
        atk_mode = self.attack_combo.currentText()
        
        for name in selected_algos:
            config = BASE_CONFIG.copy()
            config["total_episodes"] = total_ep
            config["episodes_per_batch"] = batch_ep
            config["num_obstacles"] = obs_num
            config["attack_mode"] = atk_mode
            config.update(ALGORITHM_CONFIGS[name])
            
            worker = TrainingWorker(config, name, self.data_queue, self.stop_flag)
            worker.start()
            self.running_threads[name] = worker
            self.append_log(f"â–¶ï¸ [{name}] ì‹œì‘ (ë…¼ë¬¸ ëª…ì„¸ ë²„ì „)\n")
    
    def stop_all_training(self):
        """ëª¨ë“  í•™ìŠµ ì¤‘ë‹¨"""
        self.stop_flag[0] = True
        self.append_log("âš ï¸ í•™ìŠµ ì¤‘ë‹¨ ìš”ì²­...\n")
    
    def run_demo(self):
        """í•™ìŠµëœ ëª¨ë¸ë¡œ ë°ëª¨ ì‹¤í–‰"""
        # ì•Œê³ ë¦¬ì¦˜ ì„ íƒ í™•ì¸
        selected_algos = [name for name, cb in self.algo_checkboxes.items() if cb.isChecked()]
        
        if not selected_algos:
            self.append_log("âš ï¸ ë°ëª¨ë¥¼ ì‹¤í–‰í•  ì•Œê³ ë¦¬ì¦˜ì„ ì„ íƒí•´ì£¼ì„¸ìš”.\n")
            return
        
        if len(selected_algos) > 1:
            self.append_log("âš ï¸ ë°ëª¨ëŠ” í•œ ë²ˆì— í•˜ë‚˜ì˜ ì•Œê³ ë¦¬ì¦˜ë§Œ ì‹¤í–‰ ê°€ëŠ¥í•©ë‹ˆë‹¤.\n")
            return
        
        algo_name = selected_algos[0]
        
        # ëª¨ë¸ ê²½ë¡œ ì„ íƒ ë‹¤ì´ì–¼ë¡œê·¸
        from PySide6.QtWidgets import QFileDialog
        model_dir = QFileDialog.getExistingDirectory(
            self, 
            "í•™ìŠµëœ ëª¨ë¸ í´ë” ì„ íƒ",
            "./models",
            QFileDialog.Option.ShowDirsOnly
        )
        
        if not model_dir:
            self.append_log("âš ï¸ ëª¨ë¸ í´ë”ê°€ ì„ íƒë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.\n")
            return
        
        # ë°ëª¨ ì‹¤í–‰
        self.append_log(f"ğŸ® [{algo_name}] ë°ëª¨ ì‹¤í–‰ ì¤‘...\n")
        self.append_log(f"ğŸ“ ëª¨ë¸ ê²½ë¡œ: {model_dir}\n")
        
        try:
            config = BASE_CONFIG.copy()
            config.update(ALGORITHM_CONFIGS[algo_name])
            config["render_mode"] = "human"  # ì‹œê°í™” í™œì„±í™”
            
            # ë°ëª¨ ìŠ¤ë ˆë“œ ì‹œì‘
            demo_thread = threading.Thread(
                target=self.demo_worker,
                args=(config, algo_name, model_dir),
                daemon=True
            )
            demo_thread.start()
            
        except Exception as e:
            self.append_log(f"âŒ ë°ëª¨ ì‹¤í–‰ ì‹¤íŒ¨: {e}\n")
    
    def demo_worker(self, config, algo_name, model_dir):
        """ë°ëª¨ ì‹¤í–‰ ì›Œì»¤"""
        try:
            # í™˜ê²½ ìƒì„±
            env = CTDEMultiUAVEnv(config, render_mode="human")
            agent = MAPPOAgentWithTrust(env.local_obs_dim, env.global_obs_dim, env.action_dim, config)
            
            # ëª¨ë¸ ë¡œë“œ
            try:
                agent.load_models(model_dir)
                self.data_queue.put(("log", f"âœ… ëª¨ë¸ ë¡œë“œ ì™„ë£Œ\n"))
            except Exception as e:
                self.data_queue.put(("log", f"âš ï¸ ëª¨ë¸ ë¡œë“œ ì‹¤íŒ¨, ëœë¤ ì •ì±… ì‚¬ìš©: {e}\n"))
            
            # ë°ëª¨ ì—í”¼ì†Œë“œ ì‹¤í–‰
            for ep in range(config["demo_episodes"]):
                scenario = EnvironmentScenario(config)
                lo, go = env.reset_with_scenario(scenario)
                agent.reset_episode(env.agents)
                done = False
                ep_r = 0
                step = 0
                
                self.data_queue.put(("log", f"\nğŸ“º ì—í”¼ì†Œë“œ {ep+1}/{config['demo_episodes']} ì‹œì‘\n"))
                
                while not done and step < config["max_steps"]:
                    # ê²°ì •ì  ì•¡ì…˜ ì„ íƒ (íƒí—˜ ì—†ì´)
                    acts, _, _, trust_info = agent.select_action(
                        lo, go, env.uav_positions, env.gps_positions, 
                        env=env, deterministic=True
                    )
                    
                    lo, go, rew, done, info = env.step(acts)
                    ep_r += sum(rew.values())
                    step += 1
                    
                    # ë Œë”ë§
                    env.render()
                    time.sleep(config["render_delay"])
                
                # ê²°ê³¼ ì¶œë ¥
                success_rate = info.get("success_rate", 0)
                collision_rate = info.get("collision_rate", 0)
                self.data_queue.put(("log", 
                    f"  ë³´ìƒ: {ep_r:.1f}, ì„±ê³µë¥ : {success_rate:.1%}, ì¶©ëŒë¥ : {collision_rate:.1%}\n"))
            
            env.close()
            self.data_queue.put(("log", f"\nâœ… ë°ëª¨ ì™„ë£Œ\n"))
            
        except Exception as e:
            import traceback
            self.data_queue.put(("log", f"âŒ ë°ëª¨ ì˜¤ë¥˜: {e}\n{traceback.format_exc()}\n"))
    
    def open_tensorboard(self):
        """TensorBoard ì‹¤í–‰"""
        import subprocess
        try:
            subprocess.Popen(["tensorboard", "--logdir=runs"])
            self.append_log("ğŸ“Š TensorBoard ì‹¤í–‰ ì¤‘... (http://localhost:6006)\n")
        except Exception as e:
            self.append_log(f"âŒ TensorBoard ì‹¤í–‰ ì‹¤íŒ¨: {e}\n")


def main():
    """ë©”ì¸ í•¨ìˆ˜"""
    app = QApplication(sys.argv)
    app.setStyleSheet(qdarkstyle.load_stylesheet(qt_api='pyside6'))
    
    window = MainWindow()
    window.show()
    
    sys.exit(app.exec())


if __name__ == "__main__":
    main()
