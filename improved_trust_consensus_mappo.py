# improved_trust_consensus_mappo.py
"""
GPS ìŠ¤í‘¸í•‘ í™˜ê²½ì—ì„œ ì‹ ë¢°ë„ ê¸°ë°˜ ë‹¤ì¤‘ UAV í˜‘ë ¥ ê²½ë¡œ ê³„íš
Trust-based Cooperative Path Planning for Multi-UAV Systems under GPS Spoofing Attacks

ë…¼ë¬¸ ëª…ì„¸ì— ë§ê²Œ ê°œì„ ëœ ë²„ì „
Author: ê¹€ë„ìœ¤ (ë…¼ë¬¸ ì €ì)
Improved by: AI Code Reviewer

ì£¼ìš” ê°œì„ ì‚¬í•­:
1. Trust Network ì•„í‚¤í…ì²˜: 3ì¸µ Ã— 16 ë‰´ëŸ° (ë…¼ë¬¸ ëª…ì„¸ ì¤€ìˆ˜)
2. Actor ë„¤íŠ¸ì›Œí¬: ë¶ˆí•„ìš”í•œ ì¸µ ì œê±° (1 hidden layer)
3. Consensus Protocol: 50% íˆ¬í‘œ ê¸°ë°˜ ê°•ì œ ì„¤ì • ë©”ì»¤ë‹ˆì¦˜ ì¶”ê°€
4. í•˜ì´í¼íŒŒë¼ë¯¸í„°: ë…¼ë¬¸ ëª…ì„¸ì™€ ì •í™•íˆ ì¼ì¹˜í•˜ë„ë¡ ìˆ˜ì •
5. ê´€ì°° ê³µê°„: ìœµí•©ëœ ìœ„ì¹˜ ì‚¬ìš© ë° ì†ë„ ì¶”ê°€
6. GPS ê³µê²© í™•ë¥ : 10%ë¡œ ìˆ˜ì •
"""

import os
import sys
import time
import threading
import queue
import random
import warnings
import csv
import datetime
from collections import deque
from copy import deepcopy
import numpy as np
import pygame
import torch
import torch.nn as nn
import torch.nn.functional as F
import torch.optim as optim
from torch.distributions import Categorical
from torch.utils.tensorboard import SummaryWriter
from PySide6.QtWidgets import *
from PySide6.QtCore import QTimer, Qt
from PySide6.QtGui import QTextCursor, QFont
import qdarkstyle
from matplotlib.figure import Figure
from matplotlib.backends.backend_qtagg import FigureCanvasQTAgg as FigureCanvas
import matplotlib.pyplot as plt

warnings.filterwarnings('ignore', category=UserWarning, module='matplotlib.font_manager')

# ==================== CONFIG (ë…¼ë¬¸ ëª…ì„¸ ì¤€ìˆ˜) ====================
BASE_CONFIG = {
    # ---------------- ë³´ìƒ ì„¤ì • (ìµœì í™”) ----------------
    "reward_goal": 120.0,  # ğŸ”¥ 100 â†’ 120 (ëª©í‘œ ë„ë‹¬ ê°•í•œ ë³´ìƒ)
    "reward_team_success": 30.0,
    "reward_collision": -30.0,  # ğŸ”¥ -50 â†’ -30 (ì¶©ëŒ í˜ë„í‹° ì™„í™”)
    "reward_step_penalty": -0.1,
    "distance_reward_factor": 1.5,  # ğŸ”¥ 1.0 â†’ 1.5 (ëª©í‘œ ì ‘ê·¼ ë³´ìƒ ë” ì¦ê°€)
    
    # ---------------- í•™ìŠµ í•˜ì´í¼íŒŒë¼ë¯¸í„° (ğŸ”¥ FIX: ê· í˜• í•™ìŠµ) ----------------
    "mappo_lr": 3e-4,  # ğŸ”¥ 1e-4 â†’ 3e-4 (MAPPOë„ ë¹ ë¥´ê²Œ)
    "mappo_entropy": 0.01,
    "gamma": 0.99,
    "gae_lambda": 0.95,
    "ppo_clip_epsilon": 0.2,
    "update_epochs": 10,
    "batch_size": 512,
    
    # ---------------- í™˜ê²½ ì„¤ì • (ğŸ”¥ FIX: ì¡°ê¸ˆ ë” ì‰½ê²Œ) ----------------
    "num_uavs": 5,  # ğŸ”¥ 6 â†’ 5 (ë”ë” ë‹¨ìˆœí•œ í˜‘ë ¥)
    "grid_size": 40,
    "num_obstacles": 15,  # ğŸ”¥ 20 â†’ 15 (ì¥ì• ë¬¼ ë”ë” ê°ì†Œ)
    "max_steps": 150,
    "vision_range": 6,
    
    # ---------------- ê³µê²© ì„¤ì • (ğŸ”¥ FIX: ê³µê²© ë¹„ìœ¨ ì™„í™”) ----------------
    "attack_prob": 0.02,  # ğŸ”¥ 0.05 â†’ 0.02 (ë‹¤ì‹œ ë‚®ì¶¤)
    "attack_mode": "hybrid",
    "attack_start_prob": 0.02,  # ğŸ”¥ 0.05 â†’ 0.02 (ì‹¤ì œ ~20% ê³µê²©)
    "attack_min_duration": 10,  # ğŸ”¥ 15 â†’ 10
    "attack_max_duration": 20,  # ğŸ”¥ 25 â†’ 20
    
    # ---------------- Trust Network ì„¤ì • (ğŸ”¥ FIX: ê· í˜• í•™ìŠµ) ----------------
    "use_trust_network": True,
    "trust_hidden": 32,  # ë” ê°•ë ¥í•œ ë„¤íŠ¸ì›Œí¬
    "trust_lr": 3e-4,  # ğŸ”¥ 5e-4 â†’ 3e-4 (MAPPOì™€ ê°™ì€ ì†ë„)
    "trust_lambda_reg": 0.1,  # ğŸ”¥ 0.05 â†’ 0.1 (smoothness ê°•í™”)
    "max_correction": 3.0,  # ğŸ”¥ NEW: ìµœëŒ€ ë³´ì • ë²”ìœ„ (5.0 â†’ 3.0)
    
    # ---------------- Consensus ì„¤ì • (ğŸ”¥ NEW: ë³´ì • ìŠ¤ì¼€ì¼ ì¡°ì • ë°©ì‹) ----------------
    "use_consensus": True,
    "consensus_threshold": 2.5,
    "consensus_weight": 0.08,  # ì‚¬ìš© ì•ˆ í•¨ (ë³´ì • ìŠ¤ì¼€ì¼ë¡œ ëŒ€ì²´)
    "consensus_vote_threshold": 0.5,  # ğŸ”¥ 0.7 â†’ 0.5 (50% íˆ¬í‘œë¡œ ë³µì›)
    
    # ---------------- LSTM ê¸°ë°˜ ìŠ¤í‘¸í•‘ ë³´ì •ê¸° ì„¤ì • ----------------
    "detector_seq_len": 10,
    "detector_feature_dim": 5,
    "detector_hidden": 64,
    
    # ---------------- í•™ìŠµ ì œì–´ ----------------
    "total_episodes": 10000,
    "episodes_per_batch": 10,
    "render_delay": 0.1,
    "demo_episodes": 3,
    "checkpoint_interval": 5000,
    "seed": 42,
}

DEVICE = torch.device("cuda" if torch.cuda.is_available() else "cpu")

ALGORITHM_CONFIGS = {
    "Vanilla-MAPPO": {
        "use_trust_network": False,
        "use_consensus": False,
        "use_lstm_detection": False,
        "description": "ê¸°ë³¸ MAPPO (Baseline, ë¬´ë°©ë¹„)"
    },
    "LSTM-MAPPO": {
        "use_trust_network": False,
        "use_consensus": False,
        "use_lstm_detection": True,
        "description": "LSTM ê¸°ë°˜ (ê¸°ì¡´ ì—°êµ¬, ì‹œê³„ì—´ ì˜ì¡´)"
    },
    "Trust-MAPPO": {
        "use_trust_network": True,
        "use_consensus": False,
        "use_lstm_detection": False,
        "description": "ì‹ ë¢°ë„ í•™ìŠµë§Œ (Ablation)"
    },
    "Trust+Consensus-MAPPO": {
        "use_trust_network": True,
        "use_consensus": True,
        "use_lstm_detection": False,
        "description": "ì œì•ˆ ê¸°ë²• (Ours, Full) - ë…¼ë¬¸"
    },
    "LSTM-Detector-MAPPO": {
        "use_trust_network": False,
        "use_consensus": False,
        "use_lstm_detection": False,
        "use_spoof_lstm_detector": True,
        "description": "LSTM ê¸°ë°˜ GPS ìŠ¤í‘¸í•‘ ë³´ì • baseline"
    }
}

# ==================== UTILS ====================
def create_model_folder_name(config, algorithm):
    timestamp = int(time.time())
    folder_name = f"RobustRL_{algorithm}_{config['attack_mode']}_obs{config['num_obstacles']}_{timestamp}"
    return folder_name

# ==================== NETWORKS (ë…¼ë¬¸ ëª…ì„¸ ì¤€ìˆ˜) ====================

class TrustNetwork(nn.Module):
    """
    ğŸ”¥ NEW: GPS Correction Network (Detector ë°©ì‹)
    - 3ê°œì˜ ì€ë‹‰ì¸µ, ê° 32 ë‰´ëŸ°
    - ì…ë ¥: 4ì°¨ì› (temporal_residual, spatial_discrepancy, gps_variance, vision_quality)
    - ì¶œë ¥: 2ì°¨ì› (correction_x, correction_y) - GPS ë³´ì •ê°’
    
    í•µì‹¬ ì•„ì´ë””ì–´:
    - Trust ê°€ì¤‘ì¹˜ ëŒ€ì‹  "ì–¼ë§ˆë‚˜ ë³´ì •í• ì§€" ì§ì ‘ í•™ìŠµ
    - Vision ìœ„ì¹˜ ë¬¸ì œ í•´ê²°
    - LSTM-Detectorì™€ ê³µì •í•œ ë¹„êµ
    """
    def __init__(self, hidden=32, max_correction=5.0):
        super().__init__()
        self.max_correction = max_correction  # ìµœëŒ€ ë³´ì • ë²”ìœ„ (Â±5 ì…€)
        
        self.network = nn.Sequential(
            nn.Linear(4, hidden), nn.ReLU(),      # Layer 1: 4 â†’ 32
            nn.Linear(hidden, hidden), nn.ReLU(), # Layer 2: 32 â†’ 32
            nn.Linear(hidden, hidden), nn.ReLU(), # Layer 3: 32 â†’ 32
            nn.Linear(hidden, 2),                 # Output: 32 â†’ 2 (correction_x, correction_y)
            nn.Tanh()                             # Tanhë¡œ [-1, 1] ë²”ìœ„ ì œí•œ
        )
    
    def forward(self, trust_features):
        """
        Args:
            trust_features: (batch_size, 4) tensor
                - normalized temporal residual (GPS ì˜ˆì¸¡ ì˜¤ì°¨)
                - normalized spatial discrepancy (ì´ì›ƒê³¼ì˜ ë¶ˆì¼ì¹˜)
                - GPS variance (ê³µê²© ì‹œ ë†’ìŒ)
                - Vision quality (ì´ì›ƒ ì¡´ì¬ ì—¬ë¶€)
        Returns:
            correction: (batch_size, 2) tensor [correction_x, correction_y]
                ë²”ìœ„: [-max_correction, +max_correction]
        """
        return self.network(trust_features) * self.max_correction


class TrustLoss:
    """
    ğŸ”¥ NEW: GPS Correction Loss
    Loss = MSE(corrected_pos, real_pos) + Î» * MSE(correction_t, correction_{t-1})
    
    í•µì‹¬:
    - ë³´ì •ëœ ìœ„ì¹˜ê°€ ì‹¤ì œ ìœ„ì¹˜ì— ê°€ê¹Œì›Œì§€ë„ë¡ í•™ìŠµ
    - Smoothness Regularizationìœ¼ë¡œ ê¸‰ê²©í•œ ë³€í™” ë°©ì§€
    """
    def __init__(self, lambda_reg=0.05):
        self.lambda_reg = lambda_reg
    
    def compute(self, corrected_pos, real_pos, current_correction, prev_correction):
        """
        Args:
            corrected_pos: GPS + correction (batch_size, 2)
            real_pos: ì‹¤ì œ ìœ„ì¹˜ (batch_size, 2)
            current_correction: í˜„ì¬ ë³´ì •ê°’ (batch_size, 2)
            prev_correction: ì´ì „ ë³´ì •ê°’ (batch_size, 2)
        Returns:
            total_loss: Correction Loss + Î» * Smoothness Loss
        """
        correction_loss = torch.mean((corrected_pos - real_pos) ** 2)
        smoothness_loss = torch.mean((current_correction - prev_correction) ** 2)
        return correction_loss + self.lambda_reg * smoothness_loss


class ConsensusProtocol:
    """
    âœ… ê°œì„ : ë…¼ë¬¸ ëª…ì„¸ì— ë§ê²Œ 50% íˆ¬í‘œ ê¸°ë°˜ ê°•ì œ ì„¤ì • ë©”ì»¤ë‹ˆì¦˜ ì¶”ê°€
    
    ë…¼ë¬¸ ì•Œê³ ë¦¬ì¦˜:
    1. ê° UAVëŠ” ì´ì›ƒë“¤ì˜ GPS ìœ„ì¹˜ì™€ Vision ê´€ì¸¡ì„ ë¹„êµ
    2. ë¶ˆì¼ì¹˜ê°€ thresholdë¥¼ ì´ˆê³¼í•˜ë©´ ì˜ì‹¬ í‘œ(suspicion vote) ë¶€ì—¬
    3. ì „ì²´ ì´ì›ƒì˜ 50% ì´ìƒì—ê²Œì„œ ì˜ì‹¬ í‘œë¥¼ ë°›ìœ¼ë©´ GPS ì‹ ë¢°ë„ë¥¼ ê°•ì œë¡œ 0ìœ¼ë¡œ ì„¤ì •
    """
    def __init__(self, threshold=2.5, consensus_weight=0.15, vote_threshold=0.5):
        self.threshold = threshold  # âœ… ìˆ˜ì •: 2.0 â†’ 2.5
        self.consensus_weight = consensus_weight  # âœ… ìˆ˜ì •: 0.2 â†’ 0.15
        self.vote_threshold = vote_threshold  # âœ… ì¶”ê°€: 50% íˆ¬í‘œ ì„ê³„ê°’
    
    def compute_discrepancy(self, my_vision_obs, neighbor_gps_claim):
        """ì´ì›ƒì˜ GPS ìœ„ì¹˜ì™€ ë‚´ Vision ê´€ì¸¡ ê°„ ë¶ˆì¼ì¹˜ ê³„ì‚°"""
        return np.linalg.norm(my_vision_obs - neighbor_gps_claim)
    
    def cast_votes(self, discrepancies):
        """
        âœ… ì¶”ê°€: ê° ì´ì›ƒì— ëŒ€í•œ ì˜ì‹¬ í‘œ ë¶€ì—¬
        
        Args:
            discrepancies: List of discrepancies for each neighbor
        Returns:
            suspicion_votes: List of binary votes (1 if suspicious, 0 otherwise)
        """
        votes = []
        for disc in discrepancies:
            if disc > self.threshold:
                votes.append(1)  # ì˜ì‹¬ í‘œ
            else:
                votes.append(0)  # ì •ìƒ í‘œ
        return votes
    
    def aggregate_votes(self, votes_received):
        """
        âœ… ì¶”ê°€: ë°›ì€ ì˜ì‹¬ í‘œ ì§‘ê³„ ë° ê³µê²© ì—¬ë¶€ íŒë‹¨
        
        Args:
            votes_received: Number of suspicion votes received from neighbors
            total_neighbors: Total number of neighbors
        Returns:
            is_under_attack: True if votes_received >= 50% of total_neighbors
        """
        if len(votes_received) == 0:
            return False, 0.0
        
        suspicion_ratio = sum(votes_received) / len(votes_received)
        is_under_attack = suspicion_ratio >= self.vote_threshold
        return is_under_attack, suspicion_ratio
    
    def adjust_trust(self, trust_gps, trust_vis, suspicion_ratio, force_zero=False):
        """
        âœ…âœ… ë…¼ë¬¸ ëª…ì„¸ ì •í™•íˆ ì¤€ìˆ˜
        
        Args:
            trust_gps: Current GPS trust score (from Trust Network)
            trust_vis: Current Vision trust score (from Trust Network)
            suspicion_ratio: ì˜ì‹¬ í‘œ ë¹„ìœ¨ (votes_received / total_neighbors)
            force_zero: If True, force GPS trust to 0 (50% ì´ìƒ íˆ¬í‘œ ì‹œ)
        Returns:
            adjusted_trust_gps, adjusted_trust_vis
        
        ë…¼ë¬¸ ì•Œê³ ë¦¬ì¦˜:
        - suspicion_ratio >= 0.60 (60%): GPS trust -= 0.15
        - suspicion_ratio < 0.40 (40%): GPS trust += 0.15
        - 50% ì´ìƒ ì˜ì‹¬ í‘œ: GPS trust = 0.0 (force_zero=True)
        """
        # âœ… 1ë‹¨ê³„: 50% ì´ìƒ ì˜ì‹¬ í‘œ ì‹œ ê°•ì œ ì„¤ì •
        if force_zero:
            return 0.0, 1.0
        
        # âœ… 2ë‹¨ê³„: ë…¼ë¬¸ ëª…ì„¸ëŒ€ë¡œ 60%/40% ê²½ê³„ ê¸°ë°˜ ì¡°ì •
        if suspicion_ratio >= 0.60:
            # 60% ì´ìƒ ì˜ì‹¬ í‘œ â†’ GPS ì‹ ë¢°ë„ ê°ì†Œ
            trust_gps -= self.consensus_weight  # -0.15
        elif suspicion_ratio < 0.40:
            # 40% ë¯¸ë§Œ ì˜ì‹¬ í‘œ â†’ GPS ì‹ ë¢°ë„ ì¦ê°€
            trust_gps += self.consensus_weight  # +0.15
        # 40% ~ 60% ì‚¬ì´: ì¡°ì • ì—†ìŒ
        
        # âœ… 3ë‹¨ê³„: ê²½ê³„ê°’ í´ë¦¬í•‘ [0.0, 1.0]
        trust_gps = np.clip(trust_gps, 0.0, 1.0)
        trust_vis = 1.0 - trust_gps
        
        return trust_gps, trust_vis


class LSTMSpoofDetector(nn.Module):
    """
    LSTM ê¸°ë°˜ GPS ìŠ¤í‘¸í•‘ ë³´ì •ê¸° (Baseline ë¹„êµìš©)
    Residual Learningì„ í†µí•œ ìœ„ì¹˜ ë³´ì •
    """
    def __init__(self, feature_dim=5, hidden=64):
        super().__init__()
        self.lstm = nn.LSTM(feature_dim, hidden, batch_first=True)
        self.fc = nn.Linear(hidden, 2)
        # ì‘ì€ ì´ˆê¸°ê°’ìœ¼ë¡œ ì•ˆì •ì ì¸ í•™ìŠµ
        nn.init.uniform_(self.fc.weight, -0.001, 0.001)
        nn.init.constant_(self.fc.bias, 0)

    def forward(self, x):
        """
        Args:
            x: (batch_size, seq_len, feature_dim)
        Returns:
            correction: (batch_size, 2) position correction vector
        """
        out, _ = self.lstm(x)
        return self.fc(out[:, -1, :])


class Actor(nn.Module):
    """
    âœ… ê°œì„ : ë…¼ë¬¸ ëª…ì„¸ì— ë§ê²Œ ìˆ˜ì •
    - 1ê°œì˜ ì€ë‹‰ì¸µ (128 ë‰´ëŸ°)
    - Tanh í™œì„±í™” í•¨ìˆ˜
    - LSTM ë³€í˜•ì˜ ê²½ìš° LSTM ë ˆì´ì–´ ì¶”ê°€
    """
    def __init__(self, local_dim, act_dim, hidden=128, use_lstm=False):
        super().__init__()
        self.use_lstm = use_lstm
        self.fc1 = nn.Linear(local_dim, hidden)
        if use_lstm:
            self.lstm = nn.LSTM(hidden, hidden, batch_first=True)
        # âœ… ìˆ˜ì •: fc2 ì¸µ ì œê±° (ë…¼ë¬¸ì—ëŠ” 1ê°œ ì€ë‹‰ì¸µë§Œ)
        self.head = nn.Linear(hidden, act_dim)
    
    def forward(self, x):
        x = torch.tanh(self.fc1(x))
        if self.use_lstm:
            if x.dim() == 2:
                x = x.unsqueeze(1)
            x, _ = self.lstm(x)
            x = x[:, -1, :]
        # âœ… fc2 ì œê±°ë¡œ ì¸í•œ ìˆ˜ì •
        return F.softmax(self.head(x), dim=-1)


class Critic(nn.Module):
    """
    âœ… ë…¼ë¬¸ ëª…ì„¸ ì¤€ìˆ˜
    - 2ê°œì˜ ì€ë‹‰ì¸µ, ê° 256 ë‰´ëŸ°
    - Tanh í™œì„±í™” í•¨ìˆ˜
    """
    def __init__(self, glob_dim, hidden=256):
        super().__init__()
        self.net = nn.Sequential(
            nn.Linear(glob_dim, hidden), nn.Tanh(),
            nn.Linear(hidden, hidden), nn.Tanh(),
            nn.Linear(hidden, 1)
        )
    
    def forward(self, x):
        return self.net(x)


# ==================== ENVIRONMENT ====================

class EnvironmentScenario:
    """í™˜ê²½ ì‹œë‚˜ë¦¬ì˜¤ ìƒì„± (ì¥ì• ë¬¼, ì‹œì‘/ëª©í‘œ ìœ„ì¹˜)"""
    def __init__(self, config):
        self.config = config
        self.grid_size = config["grid_size"]
        self.num_uavs = config["num_uavs"]
        self.num_obstacles = config["num_obstacles"]
        self.grid = np.zeros((self.grid_size, self.grid_size), dtype=int)
        self._place_obstacles()
        self.start_positions, self.target_positions = self._generate_start_and_targets()
    
    def _place_obstacles(self):
        count = 0
        while count < self.num_obstacles:
            r = np.random.randint(0, self.grid_size)
            c = np.random.randint(0, self.grid_size)
            if self.grid[r, c] == 0:
                self.grid[r, c] = -1
                count += 1
    
    def _generate_start_and_targets(self):
        starts = []
        targets = []
        available_cells = [(r, c) for r in range(self.grid_size) for c in range(self.grid_size) if self.grid[r, c] == 0]
        chosen = np.random.choice(len(available_cells), 2 * self.num_uavs, replace=False)
        for i in range(self.num_uavs):
            starts.append(np.array(available_cells[chosen[2*i]], dtype=float))
            targets.append(np.array(available_cells[chosen[2*i+1]], dtype=float))
        return np.array(starts), np.array(targets)


class CTDEMultiUAVEnv:
    """
    CTDE (Centralized Training, Decentralized Execution) Multi-UAV í™˜ê²½
    
    âœ… ê°œì„ ì‚¬í•­:
    - ê´€ì°° ê³µê°„ì— ìœµí•©ëœ ìœ„ì¹˜ ì‚¬ìš©
    - ì†ë„ ì •ë³´ ì¶”ê°€
    - GPS ê³µê²© í™•ë¥  10%ë¡œ ìˆ˜ì •
    - Consensus Protocol íˆ¬í‘œ ë©”ì»¤ë‹ˆì¦˜ í†µí•©
    """
    def __init__(self, config, render_mode=None):
        self.config = config
        self.num_uavs = config["num_uavs"]
        self.grid_size = config["grid_size"]
        self.max_steps = config["max_steps"]
        self.vision_range = config["vision_range"]
        self.agents = [f"uav_{i}" for i in range(self.num_uavs)]
        
        # 8ë°©í–¥ ì´ë™ (ìƒí•˜ì¢Œìš° + ëŒ€ê°ì„ )
        self.discrete_moves = np.array([
            [0,-1], [0,1], [-1,0], [1,0],  # ìƒí•˜ì¢Œìš°
            [-1,-1], [-1,1], [1,-1], [1,1]  # ëŒ€ê°ì„ 
        ], dtype=int)
        
        # âœ… ê°œì„ : ê´€ì°° ê³µê°„ ì¬ì •ì˜ (ìœµí•©ëœ ìœ„ì¹˜ + ì†ë„ ì¶”ê°€)
        # ìì‹ ì˜ ìƒíƒœ: fused_pos(2) + velocity(2) + target(2) + trust_features(4) + consensus_vote(1) = 11
        self_dim = 2 + 2 + 2 + 4 + 1
        neighbor_dim = (self.num_uavs - 1) * 5  # ê° ì´ì›ƒ: rel_pos(2) + gps_pos(2) + discrepancy(1)
        vision_dim = (2 * self.vision_range + 1) ** 2
        self.local_obs_dim = self_dim + neighbor_dim + vision_dim
        
        self.global_obs_dim = (self.num_uavs * 4) + (self.grid_size * self.grid_size)
        self.action_dim = len(self.discrete_moves)
        
        self.render_mode = render_mode
        self.window = None
        if self.render_mode == "human":
            self._init_pygame()
        
        self.consensus = ConsensusProtocol(
            self.config["consensus_threshold"], 
            self.config["consensus_weight"],
            self.config["consensus_vote_threshold"]
        )
        
        self.attack_mode = config["attack_mode"]
        self.attack_start_prob = config.get("attack_start_prob", 0.1)  # âœ… ìˆ˜ì •: 10%
        self.attack_min_duration = config.get("attack_min_duration", 10)
        self.attack_max_duration = config.get("attack_max_duration", 30)
        
        # ê³µê²© ìƒíƒœ ê´€ë¦¬
        self.attack_remaining_steps = np.zeros(self.num_uavs, dtype=int)
        self.attack_drift_dir = np.zeros((self.num_uavs, 2), dtype=float)
        self.attack_step_offset = np.zeros((self.num_uavs, 2), dtype=float)
        self.active_attack_types = ["none"] * self.num_uavs
        
        self.consensus_votes = np.zeros(self.num_uavs, dtype=float)
        self.suspicion_votes_received = [[] for _ in range(self.num_uavs)]  # âœ… ì¶”ê°€
    
    def reset_with_scenario(self, scenario):
        """ì‹œë‚˜ë¦¬ì˜¤ë¡œ í™˜ê²½ ì´ˆê¸°í™”"""
        self.current_step = 0
        self.grid = scenario.grid.copy()
        self.shared_map = np.full((self.grid_size, self.grid_size), -2.0, dtype=np.float32)
        
        self.uav_positions = scenario.start_positions.copy().astype(float)
        self.target_positions = scenario.target_positions.copy()
        self.uav_status = ["active"] * self.num_uavs
        
        self.last_positions = self.uav_positions.copy()
        self.last_velocities = np.zeros((self.num_uavs, 2))
        self.gps_positions = self.uav_positions.copy()
        self.drift_bias = np.zeros((self.num_uavs, 2))
        self.is_under_attack = [False] * self.num_uavs
        
        # ê³µê²© ìƒíƒœ ì´ˆê¸°í™”
        self.attack_remaining_steps.fill(0)
        self.attack_drift_dir.fill(0)
        self.attack_step_offset.fill(0)
        self.active_attack_types = ["none"] * self.num_uavs
        
        self.prev_distances = np.linalg.norm(self.uav_positions - self.target_positions, axis=1)
        self.step_counts = [0] * self.num_uavs
        self.total_path_lengths = [0.0] * self.num_uavs
        self.uav_paths = {agent_id: [pos.copy()] for agent_id, pos in zip(self.agents, self.uav_positions)}
        
        self.suspicion_votes_received = [[] for _ in range(self.num_uavs)]  # âœ… ì¶”ê°€
        
        self._update_shared_map()
        return self._compute_observations()
    
    def step(self, actions):
        """í™˜ê²½ ìŠ¤í… ì‹¤í–‰"""
        self.current_step += 1
        self.last_positions = self.uav_positions.copy()
        
        # ê° UAV ì´ë™
        for i, aid in enumerate(self.agents):
            if self.uav_status[i] == "active":
                move = self.discrete_moves[actions[aid]]
                self.last_velocities[i] = move
                intended = self.uav_positions[i] + move
                self.total_path_lengths[i] += np.linalg.norm(move)
                
                # ì¶©ëŒ ì²´í¬
                is_collision = False
                if not (0 <= intended[0] < self.grid_size and 0 <= intended[1] < self.grid_size):
                    is_collision = True
                elif self.grid[int(intended[1]), int(intended[0])] == -1:
                    is_collision = True
                
                if is_collision:
                    self.uav_status[i] = "collision"
                else:
                    self.uav_positions[i] = intended
                    self.uav_paths[aid].append(intended.copy())
        
        # UAV ê°„ ì¶©ëŒ ì²´í¬
        for i in range(self.num_uavs):
            if self.uav_status[i] == "active":
                for j in range(i+1, self.num_uavs):
                    if self.uav_status[j] == "active":
                        if np.array_equal(self.uav_positions[i], self.uav_positions[j]):
                            self.uav_status[i] = "collision"
                            self.uav_status[j] = "collision"
        
        # ëª©í‘œ ë„ë‹¬ ì²´í¬
        for i in range(self.num_uavs):
            if self.uav_status[i] == "active":
                if np.linalg.norm(self.uav_positions[i] - self.target_positions[i]) < 1.5:
                    self.uav_status[i] = "success"
        
        # GPS ìŠ¤í‘¸í•‘ ê³µê²© ì‹œë®¬ë ˆì´ì…˜
        self._simulate_attacks()
        
        # ê³µìœ  ë§µ ì—…ë°ì´íŠ¸
        self._update_shared_map()
        
        # ë³´ìƒ ê³„ì‚°
        rewards = self._calculate_rewards()
        
        # ê´€ì°° ê³„ì‚°
        local, global_obs = self._compute_observations()
        
        # ì¢…ë£Œ ì¡°ê±´
        done = (all(s != "active" for s in self.uav_status)) or (self.current_step >= self.max_steps)
        info = self._get_info(done)
        
        return local, global_obs, rewards, done, info
    
    def _simulate_attacks(self):
        """
        GPS ìŠ¤í‘¸í•‘ ê³µê²© ì‹œë®¬ë ˆì´ì…˜
        
        ê³µê²© íƒ€ì…:
        1. Step Attack: ê³ ì • ì˜¤í”„ì…‹ (-4.0 ~ 4.0m)
        2. Drift Attack: ëˆ„ì  í¸í–¥ (0.2 ~ 0.8 m/s)
        3. Hybrid: ëœë¤ ì„ íƒ
        """
        self.gps_positions = self.uav_positions.copy()
        self.is_under_attack = [False] * self.num_uavs
        
        if self.config["attack_mode"] == "none":
            return
        
        for i in range(self.num_uavs):
            if self.uav_status[i] != "active":
                self.attack_remaining_steps[i] = 0
                continue
            
            # ì§„í–‰ ì¤‘ì¸ ê³µê²©
            if self.attack_remaining_steps[i] > 0:
                self.is_under_attack[i] = True
                atk_type = self.active_attack_types[i]
                
                if atk_type == "step":
                    self.gps_positions[i] += self.attack_step_offset[i]
                elif atk_type == "drift":
                    noise = np.random.normal(0.0, 0.1, size=2)
                    self.drift_bias[i] += self.attack_drift_dir[i] + noise
                    self.gps_positions[i] += self.drift_bias[i]
                
                self.attack_remaining_steps[i] -= 1
            
            # ìƒˆë¡œìš´ ê³µê²© ì‹œì‘
            elif np.random.rand() < self.attack_start_prob:
                duration = np.random.randint(self.attack_min_duration, self.attack_max_duration+1)
                self.attack_remaining_steps[i] = duration
                self.is_under_attack[i] = True
                
                # ê³µê²© íƒ€ì… ì„ íƒ
                if self.attack_mode == "hybrid":
                    self.active_attack_types[i] = "step" if np.random.rand() < 0.5 else "drift"
                else:
                    self.active_attack_types[i] = self.attack_mode
                
                # ê³µê²© íŒŒë¼ë¯¸í„° ì„¤ì •
                if self.active_attack_types[i] == "step":
                    self.attack_step_offset[i] = np.random.uniform(-4.0, 4.0, size=2)
                    self.gps_positions[i] += self.attack_step_offset[i]
                elif self.active_attack_types[i] == "drift":
                    angle = np.random.uniform(0, 2*np.pi)
                    self.attack_drift_dir[i] = np.array([np.cos(angle), np.sin(angle)]) * np.random.uniform(0.2, 0.8)
                    self.drift_bias[i] = np.zeros(2)
                    self.gps_positions[i] += self.drift_bias[i]
    
    def _compute_observations(self):
        """
        âœ… ê°œì„ : ë…¼ë¬¸ ëª…ì„¸ì— ë§ê²Œ ê´€ì°° ê³µê°„ ì¬êµ¬ì„±
        
        ê° UAVì˜ ê´€ì°°:
        - ìœµí•©ëœ ìœ„ì¹˜ (fused_pos) - ì•„ì§ ìœµí•©ë˜ì§€ ì•Šì€ ê²½ìš° GPS ì‚¬ìš©
        - ì†ë„ (velocity)
        - ëª©í‘œ ìœ„ì¹˜ (target)
        - Trust features (temporal_residual, spatial_discrepancy, gps_variance, neighbor_flag)
        - Consensus vote
        - ì´ì›ƒ ì •ë³´
        - Local vision
        """
        local_obs, all_states = {}, []
        
        # ê¸€ë¡œë²Œ ìƒíƒœ êµ¬ì„±
        for i in range(self.num_uavs):
            all_states.extend(self.uav_positions[i] / self.grid_size)
            all_states.extend(self.target_positions[i] / self.grid_size)
        
        # âœ… ì¶”ê°€: Consensus Protocol íˆ¬í‘œ ìˆ˜ì§‘
        self.suspicion_votes_received = [[] for _ in range(self.num_uavs)]
        
        # ê° UAVì˜ ë¡œì»¬ ê´€ì°° ìƒì„±
        for i in range(self.num_uavs):
            # Temporal Residual: ì˜ˆì¸¡ ìœ„ì¹˜ vs GPS ìœ„ì¹˜
            pred_pos = self.last_positions[i] + self.last_velocities[i]
            temp_res = np.linalg.norm(pred_pos - self.gps_positions[i])
            
            # GPS Variance (ê³µê²© ì—¬ë¶€ì— ë”°ë¼ ë‹¤ë¦„)
            gps_var = np.random.uniform(0.1, 0.5) if not self.is_under_attack[i] else np.random.uniform(2.0, 5.0)
            
            # âœ… ê°œì„ : ì´ì›ƒ ì •ë³´ ê³ ì • ì°¨ì›ìœ¼ë¡œ ìƒì„±
            neighbor_features = []  # ê³ ì • ê¸¸ì´ ë¦¬ìŠ¤íŠ¸
            discrepancies = []
            
            for j in range(self.num_uavs):
                if i == j:
                    continue
                
                dist = np.linalg.norm(self.uav_positions[j] - self.uav_positions[i])
                if dist <= self.vision_range:
                    vis_pos = self.uav_positions[j]
                    gps_claim = self.gps_positions[j]
                    disc = np.linalg.norm(vis_pos - gps_claim)
                    discrepancies.append(disc)
                    
                    # âœ… ì¶”ê°€: íˆ¬í‘œ ìˆ˜í–‰
                    if disc > self.consensus.threshold:
                        self.suspicion_votes_received[j].append(1)
                    else:
                        self.suspicion_votes_received[j].append(0)
                    
                    # ìƒëŒ€ ìœ„ì¹˜ (2), GPS ìƒëŒ€ ìœ„ì¹˜ (2), ë¶ˆì¼ì¹˜ (1) = 5ì°¨ì›
                    rel_pos = (vis_pos - self.uav_positions[i]) / self.grid_size
                    gps_rel = (self.gps_positions[j] - self.gps_positions[i]) / self.grid_size
                    neighbor_features.extend([rel_pos[0], rel_pos[1], gps_rel[0], gps_rel[1], disc])
                else:
                    # ì´ì›ƒì´ ë²”ìœ„ ë°–: 0ìœ¼ë¡œ ì±„ì›€
                    neighbor_features.extend([0.0, 0.0, 0.0, 0.0, 0.0])
            
            # ê³ ì • ê¸¸ì´ ë³´ì¥: (num_uavs-1) * 5
            neighbor_info = np.array(neighbor_features, dtype=np.float32)
            
            # âœ…âœ… ë…¼ë¬¸ ëª…ì„¸: Consensus VoteëŠ” suspicion_ratio (ë‚´ê°€ ë°›ì€ ì˜ì‹¬ í‘œ ë¹„ìœ¨)
            my_votes = self.suspicion_votes_received[i]
            suspicion_ratio = sum(my_votes) / len(my_votes) if my_votes else 0.0
            self.consensus_votes[i] = suspicion_ratio  # íˆ¬í‘œ ë¹„ìœ¨ ì €ì¥
            
            # Spatial Discrepancy (ì´ì›ƒë“¤ê³¼ì˜ ë¶ˆì¼ì¹˜ í‰ê· )
            spat_disc = np.mean(discrepancies) if discrepancies else 0.0
            
            # Trust Features (ì •ê·œí™”)
            norm_temp = np.clip(temp_res / 2.0, 0.0, 1.0)
            norm_spat = np.clip(spat_disc / 1.0, 0.0, 1.0)
            
            trust_feats = np.array([
                norm_temp,
                norm_spat,
                gps_var,
                1.0 if discrepancies else 0.0  # ì´ì›ƒ ì¡´ì¬ ì—¬ë¶€
            ], dtype=np.float32)
            
            # âœ… ê°œì„ : ìœµí•©ëœ ìœ„ì¹˜ ì‚¬ìš© (í˜„ì¬ëŠ” GPS, Agentì—ì„œ ìœµí•© í›„ ì—…ë°ì´íŠ¸)
            # ì´ˆê¸° ìƒíƒœì—ì„œëŠ” GPS ìœ„ì¹˜ ì‚¬ìš©
            my_state = np.concatenate([
                self.gps_positions[i] / self.grid_size,  # fused_pos (ë‚˜ì¤‘ì— Agentì—ì„œ ì—…ë°ì´íŠ¸)
                self.last_velocities[i] / self.grid_size,  # âœ… ì¶”ê°€: velocity
                self.target_positions[i] / self.grid_size,
                trust_feats,
                [suspicion_ratio]  # âœ… ìˆ˜ì •: íˆ¬í‘œ ë¹„ìœ¨ (discrepancy í‰ê·  ì•„ë‹˜)
            ])
            
            local_vis = self._extract_local_vision(self.uav_positions[i])
            local_obs[self.agents[i]] = np.concatenate([my_state, neighbor_info, local_vis]).astype(np.float32)
        
        global_obs = np.concatenate([np.array(all_states), self.shared_map.flatten()]).astype(np.float32)
        return local_obs, global_obs
    
    def _extract_local_vision(self, pos):
        """ë¡œì»¬ Vision ì„¼ì„œ (ì£¼ë³€ ì¥ì• ë¬¼ ê´€ì¸¡)"""
        r = self.vision_range
        roi = np.full((2*r+1, 2*r+1), -2.0, dtype=np.float32)
        px, py = int(pos[0]), int(pos[1])
        for dy in range(-r, r+1):
            for dx in range(-r, r+1):
                ny, nx = py + dy, px + dx
                if 0 <= ny < self.grid_size and 0 <= nx < self.grid_size:
                    roi[dy+r, dx+r] = self.grid[ny, nx]
        return roi.flatten()
    
    def _update_shared_map(self):
        """ê³µìœ  ë§µ ì—…ë°ì´íŠ¸ (ê° UAVì˜ Vision ë²”ìœ„ ë‚´ ì •ë³´ ê³µìœ )"""
        for i in range(self.num_uavs):
            if self.uav_status[i] == 'active':
                px, py = int(self.uav_positions[i][0]), int(self.uav_positions[i][1])
                r = self.vision_range
                y1, y2 = max(0, py-r), min(self.grid_size, py+r+1)
                x1, x2 = max(0, px-r), min(self.grid_size, px+r+1)
                self.shared_map[y1:y2, x1:x2] = self.grid[y1:y2, x1:x2]
    
    def _calculate_rewards(self):
        """ë³´ìƒ ê³„ì‚°"""
        rewards = {}
        for i, aid in enumerate(self.agents):
            r = self.config["reward_step_penalty"]
            
            if self.uav_status[i] == "success":
                r += self.config["reward_goal"]
            elif self.uav_status[i] == "collision":
                r += self.config["reward_collision"]
            else:
                # ëª©í‘œ ì ‘ê·¼ ë³´ìƒ
                dist = np.linalg.norm(self.uav_positions[i] - self.target_positions[i])
                r += (self.prev_distances[i] - dist) * self.config["distance_reward_factor"] * 10.0
            
            rewards[aid] = r
        
        self.prev_distances = np.linalg.norm(self.uav_positions - self.target_positions, axis=1)
        return rewards
    
    def _get_info(self, done):
        """ì—í”¼ì†Œë“œ ì¢…ë£Œ ì •ë³´"""
        if not done:
            return {}
        
        s = sum(1 for st in self.uav_status if st == "success")
        c = sum(1 for st in self.uav_status if st == "collision")
        
        return {
            "success_rate": s / self.num_uavs,
            "collision_rate": c / self.num_uavs,
            "avg_path_length": np.mean(self.total_path_lengths)
        }
    
    def _init_pygame(self):
        """Pygame ì´ˆê¸°í™” (ì‹œê°í™”ìš©)"""
        pygame.init()
        self.window_size = 600
        self.cell_size = self.window_size // self.grid_size
        self.window = pygame.display.set_mode((self.window_size, self.window_size))
        pygame.display.set_caption("Multi-UAV Navigation (Improved)")
        self.uav_colors = [(255,0,0), (0,255,0), (0,0,255), (255,255,0), (255,0,255)]
        self.clock = pygame.time.Clock()
    
    def render(self):
        """í™˜ê²½ ë Œë”ë§"""
        if self.render_mode != "human":
            return
        
        self.window.fill((255, 255, 255))
        
        # ì¥ì• ë¬¼ ê·¸ë¦¬ê¸°
        for r in range(self.grid_size):
            for c in range(self.grid_size):
                if self.grid[r, c] == -1:
                    pygame.draw.rect(self.window, (50, 50, 50), 
                                   (c*self.cell_size, r*self.cell_size, 
                                    self.cell_size, self.cell_size))
        
        # UAV ê²½ë¡œ ë° ìœ„ì¹˜ ê·¸ë¦¬ê¸°
        for i in range(self.num_uavs):
            color = self.uav_colors[i % len(self.uav_colors)]
            
            # ê²½ë¡œ
            if len(self.uav_paths[self.agents[i]]) > 1:
                pts = [(p[0]*self.cell_size+self.cell_size/2, 
                       p[1]*self.cell_size+self.cell_size/2) 
                       for p in self.uav_paths[self.agents[i]]]
                pygame.draw.lines(self.window, color, False, pts, 2)
            
            # UAV ìœ„ì¹˜
            pygame.draw.circle(self.window, color, 
                             (int(self.uav_positions[i][0]*self.cell_size+self.cell_size/2),
                              int(self.uav_positions[i][1]*self.cell_size+self.cell_size/2)), 5)
        
        pygame.display.flip()
        self.clock.tick(10)
    
    def close(self):
        """í™˜ê²½ ì¢…ë£Œ"""
        if self.window:
            pygame.quit()
            self.window = None


# ==================== ROLLOUT BUFFER ====================

class RolloutBuffer:
    """ê²½í—˜ ì €ì¥ ë²„í¼"""
    def __init__(self):
        self.clear()
    
    def clear(self):
        self.obs = []
        self.glo = []
        self.act = []
        self.logp = []
        self.val = []
        self.rew = []
        self.done = []
        self.adv = []
        self.ret = []
    
    def add(self, o, g, a, l, v, r, d):
        self.obs.extend(o)
        self.glo.extend(g)
        self.act.extend(a)
        self.logp.extend(l)
        self.val.extend(v)
        self.rew.extend(r)
        self.done.extend(d)


# ==================== AGENT ====================

class MAPPOAgentWithTrust:
    """
    âœ… ê°œì„ : ë…¼ë¬¸ ëª…ì„¸ì— ë§ê²Œ Trust Network í†µí•© MAPPO Agent
    
    ì£¼ìš” ê°œì„ ì‚¬í•­:
    1. Trust Network Learning Rateë¥¼ Actorì˜ 50%ë¡œ ì„¤ì •
    2. Consensus Protocol 50% íˆ¬í‘œ ë©”ì»¤ë‹ˆì¦˜ í†µí•©
    3. ìœµí•©ëœ ìœ„ì¹˜ë¥¼ Actor ì…ë ¥ìœ¼ë¡œ ì‚¬ìš©
    4. Trust Loss lambdaë¥¼ 0.1ë¡œ ìˆ˜ì •
    """
    def __init__(self, l_dim, g_dim, a_dim, config):
        self.config = config
        self.device = DEVICE
        
        # Actor & Critic ë„¤íŠ¸ì›Œí¬
        self.actor = Actor(l_dim, a_dim, hidden=128, use_lstm=config.get("use_lstm_detection", False)).to(DEVICE)
        self.critic = Critic(g_dim, hidden=256).to(DEVICE)
        
        # âœ… ìˆ˜ì •: ë…¼ë¬¸ ëª…ì„¸ì— ë§ëŠ” Learning Rate
        self.actor_opt = optim.Adam(self.actor.parameters(), lr=config["mappo_lr"])
        self.critic_opt = optim.Adam(self.critic.parameters(), lr=config["mappo_lr"])
        
        # Trust Network
        self.use_trust = config.get("use_trust_network", False)
        self.use_consensus = config.get("use_consensus", False)
        self.use_detector = config.get("use_spoof_lstm_detector", False)
        
        if self.use_trust:
            self.trust_net = TrustNetwork(
                config["trust_hidden"], 
                config.get("max_correction", 3.0)  # ğŸ”¥ configì—ì„œ max_correction ê°€ì ¸ì˜¤ê¸°
            ).to(DEVICE)
            self.trust_opt = optim.Adam(self.trust_net.parameters(), lr=config["trust_lr"])
            self.trust_loss = TrustLoss(config["trust_lambda_reg"])
            self.last_trust_scores = {}
        
        if self.use_consensus:
            self.consensus = ConsensusProtocol(
                config["consensus_threshold"], 
                config["consensus_weight"],
                config["consensus_vote_threshold"]
            )
        
        if self.use_detector:
            self.detector = LSTMSpoofDetector(
                config["detector_feature_dim"], 
                config["detector_hidden"]
            ).to(DEVICE)
            self.det_opt = optim.Adam(self.detector.parameters(), lr=config["mappo_lr"])
            self.det_hist = {}
            self.det_buf = {"in": [], "tgt": []}
        
        self.buffer = RolloutBuffer()
        self.trust_buf = {"feat": [], "gps": [], "real": [], "prev": []}  # âœ… ìˆ˜ì •: gps ì¶”ê°€, fused/curr ì œê±°
    
    def reset_episode(self, agents):
        """ì—í”¼ì†Œë“œ ì‹œì‘ ì‹œ ì´ˆê¸°í™”"""
        if self.use_trust:
            # ğŸ”¥ NEW: ì´ˆê¸° ë³´ì •ê°’ì€ 0ìœ¼ë¡œ ì‹œì‘
            self.last_trust_scores = {a: torch.tensor([0.0, 0.0], device=DEVICE) for a in agents}
        if self.use_detector:
            self.det_hist = {a: deque(maxlen=self.config["detector_seq_len"]) for a in agents}
    
    def select_action(self, l_obs, g_obs, real_pos, gps_pos, env=None, deterministic=False):
        """
        âœ… ê°œì„ : ì•¡ì…˜ ì„ íƒ ì‹œ ìœµí•©ëœ ìœ„ì¹˜ ì‚¬ìš© ë° Consensus íˆ¬í‘œ ë©”ì»¤ë‹ˆì¦˜ ì ìš©
        
        Args:
            l_obs: ë¡œì»¬ ê´€ì°° ë”•ì…”ë„ˆë¦¬
            g_obs: ê¸€ë¡œë²Œ ê´€ì°°
            real_pos: ì‹¤ì œ ìœ„ì¹˜ (í•™ìŠµìš©)
            gps_pos: GPS ìœ„ì¹˜
            env: í™˜ê²½ ê°ì²´ (Consensus íˆ¬í‘œìš©)
            deterministic: ê²°ì •ì  ì•¡ì…˜ ì„ íƒ ì—¬ë¶€
        
        Returns:
            actions, log_probs, value, trust_info
        """
        with torch.no_grad():
            actions, log_probs, trust_info = {}, {}, {}
            g_tensor = torch.tensor(g_obs, dtype=torch.float32, device=DEVICE).unsqueeze(0)
            val = self.critic(g_tensor).item()
            
            for aid, obs in l_obs.items():
                idx = int(aid.split('_')[1])
                obs_t = torch.tensor(obs, dtype=torch.float32, device=DEVICE).unsqueeze(0)
                obs_mod = obs.copy()
                t_gps, t_vis = 1.0, 0.0
                fused_pos_np = gps_pos[idx].copy()
                
                if self.use_trust:
                    # ğŸ”¥ NEW: Trust Networkë¡œ GPS ë³´ì •ê°’ ê³„ì‚°
                    t_feat = obs_t[:, 6:10]  # trust_features (4ì°¨ì›)
                    correction = self.trust_net(t_feat).squeeze(0)  # (2,) [correction_x, correction_y]
                    
                    # ğŸ”¥ NEW: Consensus Protocolë¡œ ë³´ì •ê°’ ì¡°ì •
                    force_zero = False
                    suspicion_ratio = 0.0
                    correction_scale = 1.0
                    
                    if self.use_consensus and env is not None:
                        # ë°›ì€ ì˜ì‹¬ í‘œ ì§‘ê³„
                        votes_received = env.suspicion_votes_received[idx]
                        is_under_attack, suspicion_ratio = self.consensus.aggregate_votes(votes_received)
                        
                        if is_under_attack:
                            # 50% ì´ìƒ ì˜ì‹¬ í‘œ: ë³´ì •ê°’ì„ ê°•í•˜ê²Œ ì ìš©
                            force_zero = True
                            correction_scale = 2.0  # ë³´ì • ê°•ë„ 2ë°°
                        elif suspicion_ratio >= 0.3:
                            # 30-50% ì˜ì‹¬ í‘œ: ë³´ì •ê°’ ì¦ê°€
                            correction_scale = 1.5
                        elif suspicion_ratio < 0.1:
                            # ì˜ì‹¬ í‘œ ê±°ì˜ ì—†ìŒ: ë³´ì •ê°’ ê°ì†Œ
                            correction_scale = 0.5
                    
                    # ğŸ”¥ NEW: GPS + ë³´ì •ê°’ìœ¼ë¡œ ìµœì¢… ìœ„ì¹˜ ê³„ì‚°
                    if real_pos is not None:
                        gp = torch.tensor(gps_pos[idx], device=DEVICE, dtype=torch.float32)
                        rp = torch.tensor(real_pos[idx], device=DEVICE, dtype=torch.float32)
                        
                        # ë³´ì •ëœ ìœ„ì¹˜ ê³„ì‚° (gradient ìœ ì§€)
                        corrected_pos = gp + correction * correction_scale
                        prev_correction = self.last_trust_scores.get(aid, torch.tensor([0.0, 0.0], device=DEVICE))
                        
                        # Trust Loss ê³„ì‚°ìš© ë²„í¼ì— ì €ì¥
                        self.trust_buf['feat'].append(t_feat.squeeze(0))  # (4,)
                        self.trust_buf['gps'].append(gp)  # GPS ìœ„ì¹˜
                        self.trust_buf['real'].append(rp)  # ì‹¤ì œ ìœ„ì¹˜
                        self.trust_buf['prev'].append(prev_correction)  # ì´ì „ ë³´ì •ê°’
                        self.last_trust_scores[aid] = correction.detach()
                        
                        corrected_pos_np = corrected_pos.detach().cpu().numpy()
                    else:
                        # í‰ê°€ ëª¨ë“œ: GPS + ë³´ì •ê°’ (gradient ì—†ìŒ)
                        corrected_pos_np = gps_pos[idx] + correction.cpu().numpy() * correction_scale
                    
                    # ğŸ”¥ NEW: Actor ì…ë ¥ì— ë³´ì •ëœ ìœ„ì¹˜ ì‚¬ìš©
                    obs_mod[0:2] = corrected_pos_np / self.config["grid_size"]
                    obs_t = torch.tensor(obs_mod, dtype=torch.float32, device=DEVICE).unsqueeze(0)
                    
                    # Trust ì •ë³´ (ë³´ì • í¬ê¸°ë¥¼ gps trustë¡œ í‘œí˜„)
                    correction_magnitude = float(torch.norm(correction).item())
                    trust_info[aid] = {
                        'gps': 1.0 - min(correction_magnitude / 5.0, 1.0),  # ë³´ì • í´ìˆ˜ë¡ ì‹ ë¢°ë„ ë‚®ìŒ
                        'vis': min(correction_magnitude / 5.0, 1.0),  # ë³´ì • í¬ê¸°
                        'force_zero': force_zero
                    }
                
                elif self.use_detector:
                    # LSTM Detector ì‚¬ìš©
                    gps_norm = gps_pos[idx] / self.config["grid_size"]
                    feat = np.array([gps_norm[0], gps_norm[1], obs[6], obs[7], obs[10]], dtype=np.float32)
                    self.det_hist[aid].append(feat)
                    seq = list(self.det_hist[aid])
                    while len(seq) < self.config["detector_seq_len"]:
                        seq.insert(0, [0]*5)
                    
                    seq_t = torch.tensor(seq, dtype=torch.float32, device=DEVICE).unsqueeze(0)
                    correction = self.detector(seq_t).squeeze(0).cpu().numpy()
                    obs_mod[0:2] = gps_norm + correction
                    obs_t = torch.tensor(obs_mod, dtype=torch.float32, device=DEVICE).unsqueeze(0)
                    
                    if real_pos is not None:
                        tgt = (real_pos[idx] / self.config["grid_size"]) - gps_norm
                        self.det_buf["in"].append(seq)
                        self.det_buf["tgt"].append(tgt)
                    
                    trust_info[aid] = {'gps': 1.0, 'vis': 0.0}
                else:
                    # Trust Network ë¯¸ì‚¬ìš© (Baseline)
                    trust_info[aid] = {'gps': 1.0, 'vis': 0.0}
                
                # Actorë¡œ ì•¡ì…˜ ì„ íƒ
                probs = self.actor(obs_t)
                dist = Categorical(probs)
                act = torch.argmax(probs) if deterministic else dist.sample()
                actions[aid] = act.item()
                log_probs[aid] = dist.log_prob(act).item()
            
            return actions, log_probs, val, trust_info
    
    def compute_gae(self, next_val):
        """Generalized Advantage Estimation (GAE) ê³„ì‚°"""
        rews = torch.tensor(self.buffer.rew, dtype=torch.float32)
        vals = torch.tensor(self.buffer.val + [next_val], dtype=torch.float32)
        dones = torch.tensor(self.buffer.done, dtype=torch.float32)
        
        adv = []
        last = 0
        for t in reversed(range(len(rews))):
            delta = rews[t] + self.config["gamma"] * vals[t+1] * (1-dones[t]) - vals[t]
            last = delta + self.config["gamma"] * self.config["gae_lambda"] * (1-dones[t]) * last
            adv.insert(0, last)
        
        self.buffer.adv = adv
        self.buffer.ret = [a + v for a, v in zip(adv, vals[:-1].tolist())]
    
    def update(self):
        """PPO ì—…ë°ì´íŠ¸"""
        b_obs = torch.tensor(np.array(self.buffer.obs), dtype=torch.float32, device=DEVICE)
        b_glo = torch.tensor(np.array(self.buffer.glo), dtype=torch.float32, device=DEVICE)
        b_act = torch.tensor(self.buffer.act, dtype=torch.long, device=DEVICE)
        b_log = torch.tensor(self.buffer.logp, dtype=torch.float32, device=DEVICE)
        b_adv = torch.tensor(self.buffer.adv, dtype=torch.float32, device=DEVICE)
        b_ret = torch.tensor(self.buffer.ret, dtype=torch.float32, device=DEVICE)
        
        # PPO Update
        for _ in range(self.config["update_epochs"]):
            probs = self.actor(b_obs)
            dist = Categorical(probs)
            log_p = dist.log_prob(b_act)
            ratio = torch.exp(log_p - b_log)
            
            surr1 = ratio * b_adv
            surr2 = torch.clamp(ratio, 0.8, 1.2) * b_adv
            a_loss = -torch.min(surr1, surr2).mean()
            
            c_loss = F.mse_loss(self.critic(b_glo).squeeze(), b_ret)
            loss = a_loss + 0.5 * c_loss - self.config["mappo_entropy"] * dist.entropy().mean()
            
            self.actor_opt.zero_grad()
            self.critic_opt.zero_grad()
            loss.backward()
            self.actor_opt.step()
            self.critic_opt.step()
        
        # ğŸ”¥ NEW: Trust Network Update (GPS Correction ë°©ì‹)
        if self.use_trust and self.trust_buf['feat']:
            feat_tensor = torch.stack(self.trust_buf['feat'])  # (N, 4)
            gps_tensor = torch.stack(self.trust_buf['gps'])    # (N, 2)
            real_tensor = torch.stack(self.trust_buf['real'])  # (N, 2)
            prev_correction = torch.stack(self.trust_buf['prev'])  # (N, 2) ì´ì „ ë³´ì •ê°’
            
            # Trust Network forward (gradient í™œì„±í™”)
            correction = self.trust_net(feat_tensor)  # (N, 2) [correction_x, correction_y]
            
            # ë³´ì •ëœ ìœ„ì¹˜ ê³„ì‚°
            corrected_pos = gps_tensor + correction
            
            # ğŸ”¥ NEW Loss ê³„ì‚°: Correction Loss + Smoothness Loss
            loss = self.trust_loss.compute(corrected_pos, real_tensor, correction, prev_correction)
            
            self.trust_opt.zero_grad()
            loss.backward()
            self.trust_opt.step()
            
            self.trust_buf = {k: [] for k in self.trust_buf}
        
        # LSTM Detector Update
        if self.use_detector and self.det_buf['in']:
            inp = torch.tensor(np.array(self.det_buf['in']), dtype=torch.float32, device=DEVICE)
            tgt = torch.tensor(np.array(self.det_buf['tgt']), dtype=torch.float32, device=DEVICE)
            loss = F.mse_loss(self.detector(inp), tgt)
            
            self.det_opt.zero_grad()
            loss.backward()
            self.det_opt.step()
            
            self.det_buf = {"in": [], "tgt": []}
        
        self.buffer.clear()
    
    def save_models(self, path):
        """ëª¨ë¸ ì €ì¥"""
        os.makedirs(path, exist_ok=True)
        torch.save(self.actor.state_dict(), os.path.join(path, "actor.pth"))
        torch.save(self.critic.state_dict(), os.path.join(path, "critic.pth"))
        if self.use_trust:
            torch.save(self.trust_net.state_dict(), os.path.join(path, "trust.pth"))
        if self.use_detector:
            torch.save(self.detector.state_dict(), os.path.join(path, "detector.pth"))
    
    def load_models(self, path):
        """ëª¨ë¸ ë¡œë“œ"""
        self.actor.load_state_dict(torch.load(os.path.join(path, "actor.pth"), map_location=DEVICE))
        self.critic.load_state_dict(torch.load(os.path.join(path, "critic.pth"), map_location=DEVICE))
        if self.use_trust and os.path.exists(os.path.join(path, "trust.pth")):
            self.trust_net.load_state_dict(torch.load(os.path.join(path, "trust.pth"), map_location=DEVICE))
        if self.use_detector and os.path.exists(os.path.join(path, "detector.pth")):
            self.detector.load_state_dict(torch.load(os.path.join(path, "detector.pth"), map_location=DEVICE))


# ==================== TRAINING ====================

class TrainingWorker(threading.Thread):
    """í•™ìŠµ ì›Œì»¤ ìŠ¤ë ˆë“œ"""
    def __init__(self, config, algorithm_name, data_queue, stop_flag):
        super().__init__()
        self.config = config
        self.algorithm_name = algorithm_name
        self.data_queue = data_queue
        self.stop_flag = stop_flag
    
    def run(self):
        run_training(self.config, self.algorithm_name, self.data_queue, self.stop_flag)


def run_training(config, algorithm_name, data_queue, stop_flag):
    """
    í•™ìŠµ ì‹¤í–‰ í•¨ìˆ˜
    
    âœ… ê°œì„ : select_action í˜¸ì¶œ ì‹œ env ê°ì²´ ì „ë‹¬
    """
    try:
        np.random.seed(config['seed'])
        torch.manual_seed(config['seed'])
        
        base_folder = create_model_folder_name(config, algorithm_name)
        model_base_path = os.path.join("./models", base_folder)
        os.makedirs(model_base_path, exist_ok=True)
        writer = SummaryWriter(os.path.join("runs", base_folder))
        
        data_queue.put(("log", f"ğŸ”¥ {algorithm_name} í•™ìŠµ ì‹œì‘\n"))
        env = CTDEMultiUAVEnv(config)
        agent = MAPPOAgentWithTrust(env.local_obs_dim, env.global_obs_dim, env.action_dim, config)
        
        for ep in range(0, config["total_episodes"], config["episodes_per_batch"]):
            if stop_flag[0]:
                break
            
            rew_list, succ_list, coll_list = [], [], []
            trust_gps_list, trust_vis_list, suspicion_ratio_list = [], [], []  # âœ… ì¶”ê°€
            
            for _ in range(config["episodes_per_batch"]):
                scen = EnvironmentScenario(config)
                lo, go = env.reset_with_scenario(scen)
                agent.reset_episode(env.agents)
                done = False
                ep_r = 0
                
                # ì—í”¼ì†Œë“œ ë²„í¼
                ep_obs, ep_glo, ep_act, ep_logp, ep_val, ep_rew, ep_done = [],[],[],[],[],[],[]
                ep_trust_gps, ep_trust_vis, ep_suspicion = [], [], []  # âœ… ì¶”ê°€
                
                while not done:
                    # âœ… ê°œì„ : env ê°ì²´ë¥¼ select_actionì— ì „ë‹¬
                    acts, logs, val, trust_info = agent.select_action(
                        lo, go, env.uav_positions, env.gps_positions, env=env
                    )
                    
                    # âœ… Trust ì •ë³´ ìˆ˜ì§‘
                    for aid in env.agents:
                        if aid in trust_info:
                            ep_trust_gps.append(trust_info[aid]['gps'])
                            ep_trust_vis.append(trust_info[aid]['vis'])
                    
                    # Suspicion ratio ìˆ˜ì§‘
                    ep_suspicion.extend(env.consensus_votes.tolist())
                    
                    n_lo, n_go, rew, done, info = env.step(acts)
                    
                    ep_obs.extend([lo[a] for a in env.agents if a in acts])
                    ep_glo.extend([go for _ in acts])
                    ep_act.extend(list(acts.values()))
                    ep_logp.extend(list(logs.values()))
                    ep_val.extend([val for _ in acts])
                    ep_rew.extend(list(rew.values()))
                    ep_done.extend([done for _ in acts])
                    
                    lo, go = n_lo, n_go
                    ep_r += sum(rew.values())
                
                # ë²„í¼ì— ì¶”ê°€
                agent.buffer.add(ep_obs, ep_glo, ep_act, ep_logp, ep_val, ep_rew, ep_done)
                
                rew_list.append(ep_r)
                succ_list.append(info.get("success_rate", 0))
                coll_list.append(info.get("collision_rate", 0))
                
                # âœ… Trust í†µê³„ ìˆ˜ì§‘
                if ep_trust_gps:
                    trust_gps_list.append(np.mean(ep_trust_gps))
                    trust_vis_list.append(np.mean(ep_trust_vis))
                if ep_suspicion:
                    suspicion_ratio_list.append(np.mean(ep_suspicion))
            
            # GAE ê³„ì‚° & ì—…ë°ì´íŠ¸
            with torch.no_grad():
                next_val = agent.critic(torch.tensor(go, dtype=torch.float32, device=DEVICE).unsqueeze(0)).item()
            agent.compute_gae(next_val)
            agent.update()
            
            # ë¡œê·¸
            avg_r, avg_s, avg_c = np.mean(rew_list), np.mean(succ_list), np.mean(coll_list)
            writer.add_scalar(f"{algorithm_name}/Reward", avg_r, ep)
            writer.add_scalar(f"{algorithm_name}/Success", avg_s, ep)
            writer.add_scalar(f"{algorithm_name}/Collision", avg_c, ep)
            
            # âœ… Trust/Consensus í†µê³„ ë¡œê¹… (ì•Œê³ ë¦¬ì¦˜ ì´ë¦„ í¬í•¨)
            avg_trust_gps, avg_trust_vis, avg_suspicion = 0.0, 0.0, 0.0
            if trust_gps_list:
                avg_trust_gps = np.mean(trust_gps_list)
                avg_trust_vis = np.mean(trust_vis_list)
                writer.add_scalar(f"{algorithm_name}/Trust_GPS", avg_trust_gps, ep)
                writer.add_scalar(f"{algorithm_name}/Trust_Vision", avg_trust_vis, ep)
            if suspicion_ratio_list:
                avg_suspicion = np.mean(suspicion_ratio_list)
                writer.add_scalar(f"{algorithm_name}/Consensus_SuspicionRatio", avg_suspicion, ep)
            
            if ep % 100 == 0:
                # âœ… Trust ì •ë³´ í¬í•¨í•œ ë¡œê·¸
                log_msg = f"[{algorithm_name}] Ep {ep}: Rew {avg_r:.1f} Succ {avg_s:.1%} Coll {avg_c:.1%}"
                if trust_gps_list:
                    log_msg += f" | Trust GPS:{avg_trust_gps:.3f} Vis:{avg_trust_vis:.3f}"
                if suspicion_ratio_list:
                    log_msg += f" | Suspicion:{avg_suspicion:.3f}"
                log_msg += "\n"
                data_queue.put(("log", log_msg))
                
                data_queue.put(("graph", {
                    "algorithm": algorithm_name,
                    "rew": avg_r,
                    "succ": avg_s,
                    "coll": avg_c,
                    "drift_det": 0,
                    "path_len": 0
                }))
            
            if ep % config["checkpoint_interval"] == 0 and ep > 0:
                agent.save_models(os.path.join(model_base_path, f"ckpt_{ep}"))
        
        agent.save_models(os.path.join(model_base_path, "final"))
        data_queue.put(("log", f"âœ… [{algorithm_name}] í•™ìŠµ ì™„ë£Œ\n"))
        data_queue.put(("done", algorithm_name))
        
    except Exception as e:
        import traceback
        data_queue.put(("log", f"âŒ Error in {algorithm_name}: {e}\n{traceback.format_exc()}\n"))
    finally:
        writer.close()


# ==================== GUI ====================

class GraphCanvas(FigureCanvas):
    """í•™ìŠµ ì§„í–‰ ê·¸ë˜í”„"""
    def __init__(self, parent=None):
        self.fig = Figure()
        super().__init__(self.fig)
        self.ax = self.fig.add_subplot(111)
        self.ax.set_title("Training Progress: Reward / Success / Collision")
        self.ax.set_xlabel("Episode")
        self.ax.set_ylabel("Value")
        self.graph_data = {}
    
    def update_graph(self, algorithm, rew, succ, coll, drift_det, path_len):
        if algorithm not in self.graph_data:
            self.graph_data[algorithm] = {'x': [], 'rew': [], 'succ': []}
        
        d = self.graph_data[algorithm]
        x = len(d['x']) * 100
        d['x'].append(x)
        d['rew'].append(rew)
        d['succ'].append(succ)
        
        self.ax.clear()
        for algo, vals in self.graph_data.items():
            self.ax.plot(vals['x'], vals['rew'], label=f"{algo} (Reward)", marker='o')
            # Success rateëŠ” ë³´ì¡° ì¶•ì— í‘œì‹œ (ì„ íƒì )
        
        self.ax.legend()
        self.ax.grid(True, alpha=0.3)
        self.draw()


class MainWindow(QMainWindow):
    """ë©”ì¸ GUI ìœˆë„ìš°"""
    def __init__(self):
        super().__init__()
        self.setWindowTitle("ğŸš Trust-Consensus MAPPO - Improved (ë…¼ë¬¸ ëª…ì„¸ ì¤€ìˆ˜)")
        self.setGeometry(100, 100, 1400, 900)
        self.data_queue = queue.Queue()
        self.stop_flag = [False]
        self.running_threads = {}
        
        main_widget = QWidget()
        self.setCentralWidget(main_widget)
        main_layout = QHBoxLayout(main_widget)
        
        # ì™¼ìª½ íŒ¨ë„: ì„¤ì • ë° ì œì–´
        left_panel = QVBoxLayout()
        title = QLabel("ğŸ¯ ì‹¤í—˜ ì„¤ì • ë° ì œì–´ (ë…¼ë¬¸ ëª…ì„¸ ë²„ì „)")
        title.setFont(QFont("Arial", 14, QFont.Weight.Bold))
        left_panel.addWidget(title)
        
        # ì•Œê³ ë¦¬ì¦˜ ì„ íƒ
        algo_group = QGroupBox("ì•Œê³ ë¦¬ì¦˜ ì„ íƒ")
        algo_layout = QVBoxLayout()
        self.algo_checkboxes = {}
        for algo_name, algo_config in ALGORITHM_CONFIGS.items():
            cb = QCheckBox(f"{algo_name}: {algo_config['description']}")
            self.algo_checkboxes[algo_name] = cb
            algo_layout.addWidget(cb)
        algo_group.setLayout(algo_layout)
        left_panel.addWidget(algo_group)
        
        # ê³µê²© ëª¨ë“œ ì„ íƒ
        attack_group = QGroupBox("ê³µê²© ëª¨ë“œ")
        attack_layout = QHBoxLayout()
        self.attack_combo = QComboBox()
        self.attack_combo.addItems(["hybrid", "step", "drift", "none"])
        attack_layout.addWidget(QLabel("Attack Mode:"))
        attack_layout.addWidget(self.attack_combo)
        attack_group.setLayout(attack_layout)
        left_panel.addWidget(attack_group)
        
        # í•™ìŠµ ì„¤ì •
        config_group = QGroupBox("í•™ìŠµ ì„¤ì •")
        config_layout = QFormLayout()
        self.episode_input = QLineEdit(str(BASE_CONFIG["total_episodes"]))
        self.batch_input = QLineEdit(str(BASE_CONFIG["episodes_per_batch"]))
        self.obstacle_input = QLineEdit(str(BASE_CONFIG["num_obstacles"]))
        config_layout.addRow("ì´ Episodes:", self.episode_input)
        config_layout.addRow("Batch Episodes:", self.batch_input)
        config_layout.addRow("ì¥ì• ë¬¼ ìˆ˜:", self.obstacle_input)
        config_group.setLayout(config_layout)
        left_panel.addWidget(config_group)
        
        # ë²„íŠ¼
        btn_layout1 = QHBoxLayout()
        self.start_btn = QPushButton("ğŸš€ í•™ìŠµ ì‹œì‘")
        self.start_btn.clicked.connect(self.start_training)
        self.stop_btn = QPushButton("â¹ï¸ ì¤‘ë‹¨")
        self.stop_btn.clicked.connect(self.stop_all_training)
        
        btn_layout1.addWidget(self.start_btn)
        btn_layout1.addWidget(self.stop_btn)
        left_panel.addLayout(btn_layout1)
        
        # ë°ëª¨ ë° ë„êµ¬ ë²„íŠ¼
        btn_layout2 = QHBoxLayout()
        self.demo_btn = QPushButton("ğŸ® ë°ëª¨ ì‹¤í–‰")
        self.demo_btn.clicked.connect(self.run_demo)
        self.tb_btn = QPushButton("ğŸ“Š TensorBoard")
        self.tb_btn.clicked.connect(self.open_tensorboard)
        
        btn_layout2.addWidget(self.demo_btn)
        btn_layout2.addWidget(self.tb_btn)
        left_panel.addLayout(btn_layout2)
        
        left_panel.addStretch()
        main_layout.addLayout(left_panel, 1)
        
        # ì˜¤ë¥¸ìª½ íŒ¨ë„: ê·¸ë˜í”„ ë° ë¡œê·¸
        right_panel = QVBoxLayout()
        self.graph_canvas = GraphCanvas(self)
        right_panel.addWidget(self.graph_canvas, 2)
        
        self.log_box = QTextEdit()
        self.log_box.setReadOnly(True)
        self.log_box.setFont(QFont("Consolas", 9))
        right_panel.addWidget(self.log_box, 1)
        
        main_layout.addLayout(right_panel, 2)
        
        # íƒ€ì´ë¨¸ (í ì²˜ë¦¬)
        self.timer = QTimer()
        self.timer.timeout.connect(self.process_queue)
        self.timer.start(200)
    
    def append_log(self, text):
        """ë¡œê·¸ ì¶”ê°€"""
        self.log_box.moveCursor(QTextCursor.MoveOperation.End)
        self.log_box.insertPlainText(text)
        self.log_box.moveCursor(QTextCursor.MoveOperation.End)
    
    def process_queue(self):
        """ë°ì´í„° í ì²˜ë¦¬"""
        while not self.data_queue.empty():
            item_type, payload = self.data_queue.get()
            if item_type == "log":
                self.append_log(payload)
            elif item_type == "graph":
                algo = payload['algorithm']
                self.graph_canvas.update_graph(
                    algo,
                    payload['rew'],
                    payload['succ'],
                    payload['coll'],
                    payload['drift_det'],
                    payload['path_len']
                )
    
    def start_training(self):
        """í•™ìŠµ ì‹œì‘"""
        self.stop_flag[0] = False
        selected_algos = [name for name, cb in self.algo_checkboxes.items() if cb.isChecked()]
        
        if not selected_algos:
            self.append_log("âš ï¸ ì•Œê³ ë¦¬ì¦˜ì„ ì„ íƒí•´ì£¼ì„¸ìš”.\n")
            return
        
        total_ep = int(self.episode_input.text())
        batch_ep = int(self.batch_input.text())
        obs_num = int(self.obstacle_input.text())
        atk_mode = self.attack_combo.currentText()
        
        for name in selected_algos:
            config = BASE_CONFIG.copy()
            config["total_episodes"] = total_ep
            config["episodes_per_batch"] = batch_ep
            config["num_obstacles"] = obs_num
            config["attack_mode"] = atk_mode
            config.update(ALGORITHM_CONFIGS[name])
            
            worker = TrainingWorker(config, name, self.data_queue, self.stop_flag)
            worker.start()
            self.running_threads[name] = worker
            self.append_log(f"â–¶ï¸ [{name}] ì‹œì‘ (ë…¼ë¬¸ ëª…ì„¸ ë²„ì „)\n")
    
    def stop_all_training(self):
        """ëª¨ë“  í•™ìŠµ ì¤‘ë‹¨"""
        self.stop_flag[0] = True
        self.append_log("âš ï¸ í•™ìŠµ ì¤‘ë‹¨ ìš”ì²­...\n")
    
    def run_demo(self):
        """í•™ìŠµëœ ëª¨ë¸ë¡œ ë°ëª¨ ì‹¤í–‰"""
        # ì•Œê³ ë¦¬ì¦˜ ì„ íƒ í™•ì¸
        selected_algos = [name for name, cb in self.algo_checkboxes.items() if cb.isChecked()]
        
        if not selected_algos:
            self.append_log("âš ï¸ ë°ëª¨ë¥¼ ì‹¤í–‰í•  ì•Œê³ ë¦¬ì¦˜ì„ ì„ íƒí•´ì£¼ì„¸ìš”.\n")
            return
        
        if len(selected_algos) > 1:
            self.append_log("âš ï¸ ë°ëª¨ëŠ” í•œ ë²ˆì— í•˜ë‚˜ì˜ ì•Œê³ ë¦¬ì¦˜ë§Œ ì‹¤í–‰ ê°€ëŠ¥í•©ë‹ˆë‹¤.\n")
            return
        
        algo_name = selected_algos[0]
        
        # ëª¨ë¸ ê²½ë¡œ ì„ íƒ ë‹¤ì´ì–¼ë¡œê·¸
        from PySide6.QtWidgets import QFileDialog
        model_dir = QFileDialog.getExistingDirectory(
            self, 
            "í•™ìŠµëœ ëª¨ë¸ í´ë” ì„ íƒ",
            "./models",
            QFileDialog.Option.ShowDirsOnly
        )
        
        if not model_dir:
            self.append_log("âš ï¸ ëª¨ë¸ í´ë”ê°€ ì„ íƒë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.\n")
            return
        
        # ë°ëª¨ ì‹¤í–‰
        self.append_log(f"ğŸ® [{algo_name}] ë°ëª¨ ì‹¤í–‰ ì¤‘...\n")
        self.append_log(f"ğŸ“ ëª¨ë¸ ê²½ë¡œ: {model_dir}\n")
        
        try:
            config = BASE_CONFIG.copy()
            config.update(ALGORITHM_CONFIGS[algo_name])
            config["render_mode"] = "human"  # ì‹œê°í™” í™œì„±í™”
            
            # ë°ëª¨ ìŠ¤ë ˆë“œ ì‹œì‘
            demo_thread = threading.Thread(
                target=self.demo_worker,
                args=(config, algo_name, model_dir),
                daemon=True
            )
            demo_thread.start()
            
        except Exception as e:
            self.append_log(f"âŒ ë°ëª¨ ì‹¤í–‰ ì‹¤íŒ¨: {e}\n")
    
    def demo_worker(self, config, algo_name, model_dir):
        """ë°ëª¨ ì‹¤í–‰ ì›Œì»¤"""
        try:
            # í™˜ê²½ ìƒì„±
            env = CTDEMultiUAVEnv(config, render_mode="human")
            agent = MAPPOAgentWithTrust(env.local_obs_dim, env.global_obs_dim, env.action_dim, config)
            
            # ëª¨ë¸ ë¡œë“œ
            try:
                agent.load_models(model_dir)
                self.data_queue.put(("log", f"âœ… ëª¨ë¸ ë¡œë“œ ì™„ë£Œ\n"))
            except Exception as e:
                self.data_queue.put(("log", f"âš ï¸ ëª¨ë¸ ë¡œë“œ ì‹¤íŒ¨, ëœë¤ ì •ì±… ì‚¬ìš©: {e}\n"))
            
            # ë°ëª¨ ì—í”¼ì†Œë“œ ì‹¤í–‰
            for ep in range(config["demo_episodes"]):
                scenario = EnvironmentScenario(config)
                lo, go = env.reset_with_scenario(scenario)
                agent.reset_episode(env.agents)
                done = False
                ep_r = 0
                step = 0
                
                self.data_queue.put(("log", f"\nğŸ“º ì—í”¼ì†Œë“œ {ep+1}/{config['demo_episodes']} ì‹œì‘\n"))
                
                while not done and step < config["max_steps"]:
                    # ê²°ì •ì  ì•¡ì…˜ ì„ íƒ (íƒí—˜ ì—†ì´)
                    acts, _, _, trust_info = agent.select_action(
                        lo, go, env.uav_positions, env.gps_positions, 
                        env=env, deterministic=True
                    )
                    
                    lo, go, rew, done, info = env.step(acts)
                    ep_r += sum(rew.values())
                    step += 1
                    
                    # ë Œë”ë§
                    env.render()
                    time.sleep(config["render_delay"])
                
                # ê²°ê³¼ ì¶œë ¥
                success_rate = info.get("success_rate", 0)
                collision_rate = info.get("collision_rate", 0)
                self.data_queue.put(("log", 
                    f"  ë³´ìƒ: {ep_r:.1f}, ì„±ê³µë¥ : {success_rate:.1%}, ì¶©ëŒë¥ : {collision_rate:.1%}\n"))
            
            env.close()
            self.data_queue.put(("log", f"\nâœ… ë°ëª¨ ì™„ë£Œ\n"))
            
        except Exception as e:
            import traceback
            self.data_queue.put(("log", f"âŒ ë°ëª¨ ì˜¤ë¥˜: {e}\n{traceback.format_exc()}\n"))
    
    def open_tensorboard(self):
        """TensorBoard ì‹¤í–‰"""
        import subprocess
        try:
            subprocess.Popen(["tensorboard", "--logdir=runs"])
            self.append_log("ğŸ“Š TensorBoard ì‹¤í–‰ ì¤‘... (http://localhost:6006)\n")
        except Exception as e:
            self.append_log(f"âŒ TensorBoard ì‹¤í–‰ ì‹¤íŒ¨: {e}\n")


def main():
    """ë©”ì¸ í•¨ìˆ˜"""
    app = QApplication(sys.argv)
    app.setStyleSheet(qdarkstyle.load_stylesheet(qt_api='pyside6'))
    
    window = MainWindow()
    window.show()
    
    sys.exit(app.exec())


if __name__ == "__main__":
    main()
